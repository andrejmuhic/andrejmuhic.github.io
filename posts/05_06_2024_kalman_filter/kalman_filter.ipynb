{
 "cells": [
  {
   "cell_type": "raw",
   "id": "883f95e5-c483-4270-b199-8d6b0b47a450",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Kalman filter exposed\"\n",
    "author: \"Andrej Muhic\"\n",
    "date: \"2024-06-05\"\n",
    "categories: [State space, Kalman filter]\n",
    "image: \"https://upload.wikimedia.org/wikipedia/commons/a/a5/Basic_concept_of_Kalman_filtering.svg\"\n",
    "format:\n",
    "  html:\n",
    "    code-fold: false\n",
    "    code-tools: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5981c5d95aed79",
   "metadata": {},
   "source": [
    "## Kalman filter\n",
    "The canonical example of using Kalman filter would be kinematic motion of a car, where state consists of position, velocity and potentially acceleration and other variables. The noise is traditionally induced by random forces. The kinematic equations induce the prior on the state and thus position of the car. On top of that we are able to observe the position with some random noise by means of for example GPS and that serves as additional correction of the state distribution and this induces the posterior. The cleanest setup is Bayesian. Traditionally the noise components are constant and Gaussian. We will not add control part, which would be otherwise known acceleration induced by known forces on the car, robot, etc. \n",
    "\n",
    "* $$\\text{measurement equation} \\; \\; y_t = F s_t + \\epsilon_t,\\;\\; \\text{where } \\epsilon_t \\sim N(0, \\Omega_\\epsilon), \\text{where } F, \\Omega_\\epsilon \\text{ are known}$${#eq-measure}\n",
    "* $$\\text{transition equation} \\; \\; s_t = G s_{t-1} + \\eta_t,\\;\\; \\text{where } \\eta_t \\sim N(0, \\Omega_\\eta), , \\text{where } G, \\Omega_\\eta \\text{ are known} $${#eq-transition}\n",
    "<!-- end of list -->\n",
    "\n",
    "Let us define:\n",
    "$$\\text{full history of observations} \\; \\; \\pmb{Y}_t = \\left(y_1, y_2, \\dots, y_{t-1}, y_{t}\\right)$${#eq-history}\n",
    "All noise components are Gaussian thus the joint and all marginals, conditional marginals will be Gaussian too. This enables simple computation of the posterior. We want to find $P(s_t|Y_t).$ \n",
    "\n",
    "## Bayesian interpretation\n",
    "* $$P(y_t|s_t, \\pmb{Y}_{t-1}) = \\frac{P(y_t, s_t, \\pmb{Y}_{t-1})}{P(s_t, \\pmb{Y}_{t-1})}$$\n",
    "* $$P(s_t|\\pmb{Y}_{t-1}) = \\frac{P(s_t, \\pmb{Y}_{t-1})}{P(\\pmb{Y}_{t-1})}$$\n",
    "* $$P(y_t, \\pmb{Y}_{t-1}) = \\int P(s_t, y_t|\\pmb{Y}_{t-1}) \\;\\texttt{d} s_t \\times P(\\pmb{Y}_{t-1})$$\n",
    "* $$P(y_t| \\pmb{Y}_{t-1}) = \\int P(s_t, y_t|\\pmb{Y}_{t-1}) \\;\\texttt{d} s_t $$\n",
    "* $$P(s_t|\\pmb{Y}_t) = P(s_t|y_t, \\pmb{Y}_{t-1}) = \\frac{P(s_t, y_t, \\pmb{Y}_{t-1})}{P(y_t, \\pmb{Y}_{t-1})} = \\frac{P(y_t|s_t, \\pmb{Y}_{t-1}) \\times P(s_t|\\pmb{Y}_{t-1})}{P(y_t| \\pmb{Y}_{t-1})}$$ Let us put this together:\n",
    "* The process is initialized by specifying initial $s_0$ and $\\Sigma_0.$\n",
    "* $$s_{t-1}|\\pmb{Y}_{t-1} \\sim N(\\widehat{s}_{t-1}, \\Sigma_{t-1}),$$ where $\\widehat{s}_{t-1}$ is the mean of $s_{t-1}.$\n",
    "* Out prior from transition is: $$s_t|\\pmb{Y}_{t-1} \\sim N(G \\widehat{s}_{t-1}, R_t), $$ where $$R_t = G\\Sigma_{t-1}G^T + \\Omega_\\eta.$$\n",
    "* From observation we get: $$y_t|s_t, \\pmb{Y}_{t-1} \\sim N(Fs_t, \\Omega_\\epsilon).$$\n",
    "* Thus posterior is of the form: $$P(s_t|\\pmb{Y}_t) = \\frac{P(y_t|s_t, \\pmb{Y}_{t-1}) \\times P(s_t|\\pmb{Y}_{t-1})}{P(y_t| \\pmb{Y}_{t-1})}.$$\n",
    "It is kind of nasty to compute the posterior by hand but we know it is Gaussian distribution again!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1580ae20b6710a33",
   "metadata": {},
   "source": [
    "We will borrow the result that will greatly simplify this.\n",
    "Let $x, y$ be jointly Gaussian:\n",
    "$$\\begin{bmatrix} x \\\\ y\\end{bmatrix} \\sim N\\left(\\begin{bmatrix} \\mu_x \\\\ \\mu_y\\end{bmatrix}, \\begin{bmatrix}A & C\\\\C^T& B\\end{bmatrix}\\right) = N\\left(\\begin{bmatrix} \\mu_x \\\\ \\mu_y\\end{bmatrix}, \\begin{bmatrix}\\widetilde{A} & \\widetilde{C}\\\\\\widetilde{C}^T& \\widetilde{B}\\end{bmatrix}^{-1}\\right),$$\n",
    "then the marginal distribution of $x$ and $x|y$ are:\n",
    "$$x \\sim N(\\mu_x, A), \\text{ and } x|y \\sim N(\\mu_x + CB^{-1}(y-\\mu_y), A-CB^{-1}C^T)$$ or\n",
    "$$x|y \\sim N(\\mu_x - \\widetilde{A}^{-1}\\widetilde{C}(y-\\mu_y), \\widetilde{A}^{-1})$$\n",
    "How does this help us.\n",
    "Let us write in block form:\n",
    "$$\\begin{bmatrix} s_t \\\\ y_t \\end{bmatrix} | \\pmb{Y}_{t-1}= \\begin{bmatrix} G & I & 0\\\\ FG & F & I \\end{bmatrix} \\begin{bmatrix} s_{t-1} \\\\ \\eta_t \\\\ \\epsilon_t \\end{bmatrix} | \\pmb{Y}_{t-1}$$\n",
    "We also know:\n",
    "$$\\begin{bmatrix} s_{t-1} \\\\ \\eta_t \\\\ \\epsilon_t \\end{bmatrix} | \\pmb{Y}_{t-1} =  \\begin{bmatrix}s_{t-1}|\\pmb{Y}_{t-1} \\\\ \\eta_t \\\\ \\epsilon_t \\end{bmatrix} \\sim N\\left(\\begin{bmatrix} \\widehat{s}_{t-1}\\\\ 0 \\\\0\\end{bmatrix}, \\begin{bmatrix} \\Sigma_{t-1} & 0 & 0\\\\ 0 & \\Omega_{\\eta} & 0\\\\ 0 & 0 & \\Omega_{\\epsilon}\\end{bmatrix}\\right) $$\n",
    "We do block multiplication and use the fact that $\\texttt{var}(M v) = M \\texttt{var}(v) M^T$ obtain:\n",
    "$$\\begin{bmatrix} s_t \\\\ y_t \\end{bmatrix} | \\pmb{Y}_{t-1}  \\sim N\\left(\\begin{bmatrix} G \\widehat{s}_{t-1} \\\\  FG \\widehat{s}_{t-1}\\end{bmatrix}, \\begin{bmatrix} G & I & 0\\\\ FG & F & I \\end{bmatrix} \\begin{bmatrix} \\Sigma_{t-1} & 0 & 0\\\\ 0 & \\Omega_{\\eta} & 0\\\\ 0 & 0 & \\Omega_{\\epsilon} \\end{bmatrix} \\begin{bmatrix} G & I & 0\\\\ FG & F & I \\end{bmatrix}^T \\right)=$$\n",
    "Let us define:\n",
    "$$R_t = G \\Sigma_{t-1} G^T + \\Omega_{\\eta},$$ then continue:\n",
    "$$\\begin{bmatrix} s_t \\\\ y_t \\end{bmatrix} | \\pmb{Y}_{t-1}  \\sim N\\left(\\begin{bmatrix} G \\widehat{s}_{t-1} \\\\  FG \\widehat{s}_{t-1}\\end{bmatrix}, \\begin{bmatrix} R_{t} & R_{t} F^T\\\\ FR_{t-1}^T & FR_{t}F^T + \\Omega_\\epsilon \\end{bmatrix} \\right).$$\n",
    "Now we are ready to compute:\n",
    "$$s_t| \\pmb{Y}_{t-1} | y_t \\sim N\\left(G \\widehat{s}_{t-1} + R_{t}F^T (FR_{t}F^T + \\Omega_\\epsilon)^{-1}(y_t - G \\widehat{s}_{t-1}) , R_t -  R_t F^T (FR_tF^T+\\Omega_\\epsilon)^{-1} F R_t^T\\right). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e3c857-929b-4a54-b346-a0379c710b09",
   "metadata": {},
   "source": [
    "## Simple two dimensional movement example\n",
    "\n",
    "Let us provide an example of this as two dimensional movement, where $x$ denotes position and $v$ velocity. The system is exposed to random forces. \n",
    "\n",
    "$$s_t = \\begin{bmatrix} x_1 \\\\ x_2\\\\ v_1 \\\\ v_2\\end{bmatrix}.$$\n",
    "The transtion equations are $$x_1' = x_1 + v_1 \\texttt{dt} \\times  + a_1 \\frac{\\texttt{dt}^2}{2}  \\text{ and } x_2' = x_2 + \\texttt{dt} \\times v_2 + a_2 \\frac{\\texttt{dt}^2}{2}.$$\n",
    "Let us assume that all acceleration comes from independent random external forces.\n",
    "The $s_t' = s_{t+1}$ is short hand notation and acts on all vector components.\n",
    "This can be rewritten in matrix form as:\n",
    "$$ s_t' = \\begin{bmatrix} x_1 \\\\ x_2\\\\ v_1 \\\\ v_2\\end{bmatrix}' = \\overbrace{\\begin{bmatrix} 1 & 0 & \\texttt{dt} & 0\\\\0 & 1 & 0 & \\texttt{dt}\\\\0 & 0 &1 & 0 \\\\ 0 & 0 & 0 & 1\\end{bmatrix}}^G s_t + \\overbrace{\\begin{bmatrix} \\frac{\\texttt{dt}^2}{2} & 0\\\\ 0 & \\frac{\\texttt{dt}^2}{2}\\\\ \\texttt{dt} & 0 \\\\ 0 &\\texttt{dt}\\end{bmatrix}}^{\\texttt{denote } V \\texttt{ such that } \\eta_{t+1} \\sim N(0, VV^T)} \\overbrace{\\begin{bmatrix} a_1 \\\\ a_2 \\end{bmatrix}}^{\\sim N(0,I_2)}.$$\n",
    "Expanding all this:\n",
    "$$ s_{t+1} = G s_t + \\eta_t, \\text{ where } \\eta_{t+1} = \\begin{bmatrix} \\frac{\\texttt{dt}^4}{4} & 0 & \\frac{\\texttt{dt}^3}{2} & 0\\\\ 0 & \\frac{\\texttt{dt}^4}{4} & 0 & \\frac{\\texttt{dt}^3}{2}\\\\ \\frac{\\texttt{dt}^3}{2} & 0 & \\texttt{dt}^2 & 0 \\\\ 0 &\\frac{\\texttt{dt}^3}{2} & 0 &\\texttt{dt}^2\\end{bmatrix}$$\n",
    "The observation equation is simply:\n",
    "$$y_{t+1} = y_t' = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}' =  \\overbrace{\\begin{bmatrix} 1 & 0 & 0 & 0\\\\ 0 & 0 & 1 & 0 \\end{bmatrix}}^F \\overbrace{\\begin{bmatrix} x_1 \\\\ v_1\\\\ x_2 \\\\ v_2\\end{bmatrix}}^{s_t} + \\overbrace{\\epsilon_t}^{\\sim N(0, \\frac{I_2}{4})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bd0ed3-73c0-4dca-a6dc-82b555e2970d",
   "metadata": {},
   "source": [
    "## Simple movement equation example in python\n",
    "\n",
    "There are several options for estimation:\n",
    "\n",
    "- From scratch\n",
    "- [Statsmodels statespace](https://www.statsmodels.org/stable/statespace.html#models-and-estimation)\n",
    "- [pykalman](https://github.com/pykalman/pykalman)\n",
    "- [pymc](https://www.pymc.io/projects/docs/en/stable/learn.html) [pymc experimental statsmodels](https://discourse.pymc.io/t/pymc-experimental-now-includes-state-spaces-models/12773)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de635777-dc22-4d64-9009-6e749f312182",
   "metadata": {},
   "source": [
    "### Parameters and data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc07b7563c085925",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T07:27:14.089348Z",
     "start_time": "2024-06-04T07:27:13.157520Z"
    }
   },
   "outputs": [],
   "source": [
    "# No control input in this example but that is just constant vector dependent on t\n",
    "import numpy as np\n",
    "from numpy import linalg\n",
    "import pandas as pd  # Pandas isn't necessary, but makes some output nicer\n",
    "import statsmodels.api as sm\n",
    "from pathlib import Path\n",
    "path_csv = Path('./measurements_2d.csv')\n",
    "\n",
    "Y_pandas = pd.read_csv(path_csv, index_col=0)\n",
    "Y = Y_pandas.values\n",
    "\n",
    "\n",
    "# The matrices in the dynamic model are set up as follows\n",
    "omega_eta, dt, omega_eps = 1, 0.1, 0.5\n",
    "G = np.array([[1, 0, dt, 0],\n",
    "              [0, 1, 0, dt],\n",
    "              [0, 0, 1, 0],\n",
    "              [0, 0, 0, 1]])\n",
    "Omega_custom = omega_eta ** 2 * np.array([[dt ** 3 / 3, dt ** 2 / 2, 0, 0],\n",
    "                                          [dt ** 2 / 2, dt, 0, 0],\n",
    "                                          [0, 0, dt ** 3 / 3, dt ** 2 / 2],\n",
    "                                          [0, 0, dt ** 2 / 2, dt]])\n",
    "Omega_eta = omega_eta ** 2 * np.array([[dt ** 4 / 4, 0, dt ** 3 / 2, 0],\n",
    "                                       [0, dt ** 4 / 4, 0, dt ** 3 / 2],\n",
    "                                       [dt ** 3 / 2, 0, dt ** 2, 0],\n",
    "                                       [0, dt ** 3 / 2, 0, dt ** 2]])\n",
    "# Matrices in the measurement model are designed as follows\n",
    "F = np.array([[1, 0, 0, 0],\n",
    "              [0, 1, 0, 0]])\n",
    "Omega_epsilon = omega_eps ** 2 * np.eye(2)\n",
    "# Starting values\n",
    "# Initial state vector\n",
    "s0 = np.array([[0.0, 0.0, 1.0, -1.0]]).T  # column vector\n",
    "# Initial noise covariance\n",
    "Sigma0 = np.eye(4)\n",
    "\n",
    "n = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22914bc2-c854-44ae-bfc6-adf57b862fcb",
   "metadata": {},
   "source": [
    "### Manual Kalman filter, would need numerical improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3430f01-390a-4be4-912a-f22a3689a212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           x1         x2    dx1/dt    dx2/dt\n",
      "0   -0.281083  -0.235580  0.962081 -1.013491\n",
      "1    0.100219  -0.200777  1.122475 -0.936892\n",
      "2    0.228852  -0.735516  1.141854 -1.458522\n",
      "3    0.379437  -0.749947  1.202244 -1.240481\n",
      "4    0.587982  -0.449752  1.367730 -0.445575\n",
      "..        ...        ...       ...       ...\n",
      "95  11.935788  14.163066  0.888412  1.743867\n",
      "96  12.036713  14.317419  0.900481  1.723859\n",
      "97  12.261151  14.588231  1.034703  1.822161\n",
      "98  12.322096  14.765653  0.992230  1.817373\n",
      "99  12.501377  14.992161  1.072189  1.862088\n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_786442/1222645060.py:8: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  K = linalg.lstsq(S.T, (R_t @ F.T).T)[0].T\n"
     ]
    }
   ],
   "source": [
    "s = s0\n",
    "Sigma = Sigma0\n",
    "kf_mean = np.zeros((s.shape[0], n))\n",
    "kf_Sigma = np.zeros((Sigma.shape[0], Sigma.shape[1], n))\n",
    "for t in range(100):\n",
    "    R_t = G @ Sigma @ G.T + Omega_eta\n",
    "    S = F @ R_t @ F.T + Omega_epsilon\n",
    "    K = linalg.lstsq(S.T, (R_t @ F.T).T)[0].T\n",
    "    # K = (R_t @ F.T) @ np.linalg.pinv(S)\n",
    "    s_t = G @ s\n",
    "    s = s_t + K @ (Y[:, t, np.newaxis] - F @ s_t)\n",
    "    Sigma = R_t- K @ S @ K.T\n",
    "\n",
    "    kf_mean[:, t] = s.flatten()\n",
    "    kf_Sigma[:, :, t] = Sigma\n",
    "print(pd.DataFrame(kf_mean.T, columns=['x1', 'x2', 'dx1/dt', 'dx2/dt']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd761b46-48ea-48d5-8a8d-bfea2f0327a3",
   "metadata": {},
   "source": [
    "### Kalman filter using statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b20f8b4-8678-43ab-ac5b-ac194f16d151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           x1         x2    dx1/dt    dx2/dt\n",
      "0   -0.281083  -0.235580  0.962081 -1.013491\n",
      "1    0.100219  -0.200777  1.122475 -0.936892\n",
      "2    0.228852  -0.735516  1.141854 -1.458522\n",
      "3    0.379437  -0.749947  1.202244 -1.240481\n",
      "4    0.587982  -0.449752  1.367730 -0.445575\n",
      "..        ...        ...       ...       ...\n",
      "95  11.935788  14.163066  0.888412  1.743867\n",
      "96  12.036713  14.317419  0.900481  1.723859\n",
      "97  12.261151  14.588231  1.034703  1.822161\n",
      "98  12.322096  14.765653  0.992230  1.817373\n",
      "99  12.501377  14.992161  1.072189  1.862088\n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Now instantiate a statespace model with the data\n",
    "# (data should be shaped nobs x n_variables))\n",
    "kf = sm.tsa.statespace.MLEModel(pd.DataFrame(Y.T), k_states=4)\n",
    "kf._state_names = ['x1', 'x2', 'dx1/dt', 'dx2/dt']\n",
    "kf['design'] = F\n",
    "kf['obs_cov'] = Omega_epsilon\n",
    "kf['transition'] = G\n",
    "kf['selection'] = np.eye(4)\n",
    "kf['state_cov'] = Omega_eta\n",
    "\n",
    "# Unfortunate difference in timing convetions\n",
    "# https://www.statsmodels.org/stable/statespace.html#models-and-estimation\n",
    "kf.initialize_known(G @ s0[:, 0], G @ Sigma0 @ G.T + Omega_eta)\n",
    "# To performan Kalman filtering and smoothing, use:\n",
    "res = kf.smooth([])\n",
    "# Display filtered\n",
    "print(res.states.filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c671bb-889a-46de-92f4-de9b66a5db60",
   "metadata": {},
   "source": [
    "### Manual verification of first step using pymc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a28f5c804c0f416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [ss]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac65e1d94724c82b8bb02d8b8b6ae19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 1_000 tune and 3_000 draw iterations (4_000 + 12_000 draws total) took 6 seconds.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ebbd5584784a3481b6a3b1df400fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         x1       x2    dx1/dt    dx2/dt\n",
      "0 -0.281083 -0.23558  0.962081 -1.013491\n"
     ]
    }
   ],
   "source": [
    "import pymc as pm\n",
    "\n",
    "with pm.Model() as model_multi:  # model specifications in PyMC are wrapped in a with-statement\n",
    "    ss = pm.MvNormal(\"ss\", mu=G@s0[:, 0], cov=G@Sigma0@G.T + Omega_eta, transform=None)\n",
    "    # Define likelihood\n",
    "    likelihood = pm.MvNormal(\"y\", mu=F @ ss, cov=Omega_epsilon, observed=Y[:, [0]].T)\n",
    "\n",
    "    # Inference!\n",
    "    # draw 3000 posterior samples using NUTS sampling\n",
    "    idata = pm.sample(draws=3000, chains=4)\n",
    "    # find MAP\n",
    "    res_MAP = pm.find_MAP(maxeval=50000, return_raw=True)\n",
    "print(pd.DataFrame(np.array([res_MAP[0]['ss']]), columns=['x1', 'x2', 'dx1/dt', 'dx2/dt']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
