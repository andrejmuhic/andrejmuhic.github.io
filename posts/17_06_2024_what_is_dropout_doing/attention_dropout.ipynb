{
 "cells": [
  {
   "cell_type": "raw",
   "id": "883f95e5-c483-4270-b199-8d6b0b47a450",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Attention dropout\"\n",
    "author: \"Andrej Muhic\"\n",
    "date: \"2024-06-17\"\n",
    "categories: [attention, dropout]\n",
    "format:\n",
    "  html:\n",
    "    code-fold: false\n",
    "    code-tools: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d07df1-a2c1-4475-b123-c6773de0099b",
   "metadata": {},
   "source": [
    "### Self attention softmax weights dropout\n",
    "* In the self attention head we apply dropout on softmax weights. This means that the representation of the first word vector in batch can be $0$ with dropout probability $p$?\n",
    "* Moreover, even the representation of the n-th word is 0 with probability $p^n$?\n",
    "* Should be bothered with this, maybe?\n",
    "* If we are using multiple layers and multiple heads the \"bias\" induced by this seems to be negligible.\n",
    "* If we do toy examples this actually seems to slow down the training.\n",
    "\n",
    "* Another separate question is why not to drop whole words, I assume that this is valid but it will make batch generation ragged and harder to parallelize, I would also assume that this would slow down the training as it is a lot more aggressive than dropping weights.\n",
    "* I found this one kind of surprising when I first noticed it. I would welcome any references to this and what is the reason that sampling is not modified to guarantee for example at least one nonzero weight that would ensure nonzero vector?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14071bcc-d9ef-4cf7-84b1-38f741185536",
   "metadata": {},
   "source": [
    "### Toy code example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dde051c-6c93-476b-94ab-7269e0d625da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.functional import F\n",
    "n_embd = 20\n",
    "head_size = 10\n",
    "block_size = 4\n",
    "dropout = 0.5\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention\n",
    "        https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        # This limits us to the maximal context block_size\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)  # (B,T,C=n_embd) -> (B,T,C=head_size)\n",
    "        q = self.query(x)  # (B,T,C=n_embd) -> (B,T,C=head_size)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) * C ** -0.5  # (B, T, C=head_size) @ (B, C=head_size), T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        # The drop out is over full matrix, alternatively it would be better to just drop on mask, this is biased\n",
    "        # Also it seems maybe conceptually we should just do symmetric dropout\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B,T,C=head_size)\n",
    "        # The matrix multiplication is batched and applied on last two dimensions!\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, C=head_size) -> (B, T, C=head_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1f0d786d-4f33-4c83-a5d2-4ab1100a6085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.4227, 1.1676, 0.1916, 0.7144, 1.0432, 0.5021, 0.1680, 0.1247, 1.1446,\n",
       "         0.2948],\n",
       "        [2.3534, 2.1808, 1.3240, 2.1463, 2.5636, 0.9744, 2.2360, 1.6751, 1.3820,\n",
       "         2.3454]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(1367149)\n",
    "mask = torch.tril(torch.ones(4, 4))\n",
    "similarities = torch.rand((4, 4)) * mask\n",
    "dd_similarities = torch.nn.functional.dropout(similarities, p=0.5)\n",
    "values = torch.rand((4, 10))\n",
    "dd_similarities @ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74129d62-02aa-4948-9f1c-ee5749523d49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
