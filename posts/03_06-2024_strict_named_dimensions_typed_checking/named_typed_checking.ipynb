{
 "cells": [
  {
   "cell_type": "raw",
   "id": "883f95e5-c483-4270-b199-8d6b0b47a450",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Strict typed named dimension checking\"\n",
    "author: \"Andrej Muhic\"\n",
    "date: \"2024-06-03\"\n",
    "categories: [Engineering, Coding]\n",
    "image: \"decorated_named_dimensions.png\"\n",
    "format:\n",
    "  html:\n",
    "    code-fold: false\n",
    "    code-tools: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e9e982-ea4d-42d0-aba6-0a692fc27089",
   "metadata": {},
   "source": [
    "# Named dimensions\n",
    "- [PyTorch named tensor](https://pytorch.org/docs/stable/named_tensor.html)\n",
    "- [Jax Typing](https://docs.kidger.site/jaxtyping/) [Line annotations does not work as expected yet.](https://github.com/patrick-kidger/jaxtyping/issues/153)\n",
    "\n",
    "We will look at Jax Typing and how to do named dimension checking with a code example. This is really useful as the code is much easier to read and on top of that input and output dimensions are checked. This is probably only real option currently as sadly Pytorch named tensor project is mainly abandoned? Let us proceed to code example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfcaae38-8c79-4d74-bbf4-0f562d248511",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T06:32:35.903868Z",
     "start_time": "2024-06-04T06:32:35.450770Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeCheckError",
     "evalue": "Type-check error whilst checking the return value of forward_broken.\nActual value: ( tensor([[ 0.7932,  1.2100, -0.0075,  ..., -1.1108, -0.0285, -0.5872],\n        [ 1.2552,  0.1470, -1.0430,  ...,  0.5065, -0.6637,  0.9305],\n        [-0.6178,  0.4888, -1.0378,  ...,  2.0027,  1.0175, -1.0587],\n        ...,\n        [-1.5059, -0.8984, -1.0045,  ..., -0.0105,  0.7734,  0.7782],\n        [-0.9111,  0.1937, -2.5012,  ...,  0.4195, -0.7629, -0.6584],\n        [ 0.1368, -0.4695, -0.3229,  ..., -0.6181,  0.5164,  1.2186]],\n       grad_fn=<ViewBackward0>),\n  tensor(4.6550, grad_fn=<NllLossBackward0>))\nExpected type: typing.Tuple[Float[Tensor, 'batch_dim context_dim latent_dim'], Float[Tensor, '']].\n----------------------\nCalled with parameters: { 'idx': tensor([[63, 25,  9,  ..., 28,  0, 61],\n        [ 8, 37, 18,  ..., 57,  5, 39],\n        [50, 13,  0,  ..., 21, 33, 45],\n        ...,\n        [35, 16, 49,  ..., 54, 11, 20],\n        [22, 34, 51,  ...,  9, 25, 62],\n        [64, 35,  6,  ..., 62, 10, 58]]),\n  'targets': tensor([[16, 63, 24,  ...,  8,  9, 46],\n        [ 0, 15,  8,  ..., 31,  5, 22],\n        [ 2, 32, 40,  ..., 23,  0, 18],\n        ...,\n        [31, 47, 10,  ..., 31,  4, 15],\n        [32, 57, 46,  ..., 31, 24, 45],\n        [60, 50,  3,  ..., 36, 53, 50]])}\nParameter annotations: (idx: Int64[Tensor, 'batch_dim context_dim'], targets: Int64[Tensor, 'batch_dim context_dim']).\nThe current values for each jaxtyping axis annotation are as follows.\nbatch_dim=256\ncontext_dim=65",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBeartypeCallHintReturnViolation\u001b[0m           Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jaxtyping/_decorator.py:472\u001b[0m, in \u001b[0;36mjaxtyped.<locals>.wrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 472\u001b[0m     full_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m AnnotationError:\n",
      "File \u001b[0;32m<@beartype(__main__.check_return) at 0x7fe750a3fb00>:78\u001b[0m, in \u001b[0;36mcheck_return\u001b[0;34m(__beartype_object_136591936, __beartype_get_violation, __beartype_conf, __beartype_object_136011088, __beartype_object_136012848, __beartype_func, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mBeartypeCallHintReturnViolation\u001b[0m: Function __main__.check_return() return (tensor([[ 0.7932,  1.2100, -0.0075,  ..., -1.1108, -0.0285, -0.5872],\n        [ 1.2552,  ...>)) violates type hint typing.Tuple[jaxtyping.Float[Tensor, 'batch_dim context_dim latent_dim'], jaxtyping.Float[Tensor, '']], as tuple index 0 item this array has 2 dimensions, not the 3 expected by the type hint.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeCheckError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 68\u001b[0m\n\u001b[1;32m     66\u001b[0m logits_not_none, loss_not_none \u001b[38;5;241m=\u001b[39m forward(idx, targets)\n\u001b[1;32m     67\u001b[0m logits_none, loss_none \u001b[38;5;241m=\u001b[39m forward(idx, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 68\u001b[0m logits_broken, loss_broken \u001b[38;5;241m=\u001b[39m forward_broken(idx, targets)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jaxtyping/_decorator.py:503\u001b[0m, in \u001b[0;36mjaxtyped.<locals>.wrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m TypeCheckError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    502\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 503\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m TypeCheckError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mTypeCheckError\u001b[0m: Type-check error whilst checking the return value of forward_broken.\nActual value: ( tensor([[ 0.7932,  1.2100, -0.0075,  ..., -1.1108, -0.0285, -0.5872],\n        [ 1.2552,  0.1470, -1.0430,  ...,  0.5065, -0.6637,  0.9305],\n        [-0.6178,  0.4888, -1.0378,  ...,  2.0027,  1.0175, -1.0587],\n        ...,\n        [-1.5059, -0.8984, -1.0045,  ..., -0.0105,  0.7734,  0.7782],\n        [-0.9111,  0.1937, -2.5012,  ...,  0.4195, -0.7629, -0.6584],\n        [ 0.1368, -0.4695, -0.3229,  ..., -0.6181,  0.5164,  1.2186]],\n       grad_fn=<ViewBackward0>),\n  tensor(4.6550, grad_fn=<NllLossBackward0>))\nExpected type: typing.Tuple[Float[Tensor, 'batch_dim context_dim latent_dim'], Float[Tensor, '']].\n----------------------\nCalled with parameters: { 'idx': tensor([[63, 25,  9,  ..., 28,  0, 61],\n        [ 8, 37, 18,  ..., 57,  5, 39],\n        [50, 13,  0,  ..., 21, 33, 45],\n        ...,\n        [35, 16, 49,  ..., 54, 11, 20],\n        [22, 34, 51,  ...,  9, 25, 62],\n        [64, 35,  6,  ..., 62, 10, 58]]),\n  'targets': tensor([[16, 63, 24,  ...,  8,  9, 46],\n        [ 0, 15,  8,  ..., 31,  5, 22],\n        [ 2, 32, 40,  ..., 23,  0, 18],\n        ...,\n        [31, 47, 10,  ..., 31,  4, 15],\n        [32, 57, 46,  ..., 31, 24, 45],\n        [60, 50,  3,  ..., 36, 53, 50]])}\nParameter annotations: (idx: Int64[Tensor, 'batch_dim context_dim'], targets: Int64[Tensor, 'batch_dim context_dim']).\nThe current values for each jaxtyping axis annotation are as follows.\nbatch_dim=256\ncontext_dim=65"
     ]
    }
   ],
   "source": [
    "import typing\n",
    "from typing import Optional, Union, Tuple\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "from jaxtyping import Float, Int64, jaxtyped\n",
    "# Use your favourite typechecker: usually one of the two lines below.\n",
    "import beartype\n",
    "from typeguard import typechecked as typechecker\n",
    "%reload_ext jaxtyping\n",
    "# beartype does random sampling for this example we want statis\n",
    "# %jaxtyping.typechecker beartype.beartype\n",
    "%jaxtyping.typechecker typechecker\n",
    "#jaxtyping.install_import_hook(module, typechecker=beartype.beartype)\n",
    "\n",
    "Array: typing.TypeAlias = torch.Tensor\n",
    "Long: typing.TypeAlias = Int64\n",
    "\n",
    "vocab_size = 65\n",
    "token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "\n",
    "#@jaxtyped(typechecker=typechecker)\n",
    "def forward(idx: Long[torch.Tensor, \"batch_dim context_dim\"],\n",
    "            targets: Optional[Long[torch.Tensor, \"batch_dim context_dim\"]] = None) -> Tuple[\n",
    "    Union[Float[torch.Tensor, \"batch_dim*context_dim latent_dim\"], Float[\n",
    "        torch.Tensor, \"batch_dim context_dim latent_dim\"]], Union[Float[torch.Tensor, \"\"], None]]:\n",
    "    # idx and targets are both (B,T) tensor of integers\n",
    "    logits: Long[torch.Tensor, \"batch_dim context_dim latent_dim\"] = token_embedding_table(\n",
    "        idx)  # (B,T,C=vocab_size)\n",
    "    if targets is None:\n",
    "        return logits, None\n",
    "    else:\n",
    "        # Note that here strictly speaking this does not fix batch size explicitly to B\n",
    "        B, T, C = logits.shape  # (B,T,C=vocab_size)\n",
    "        # Just a hack to avoid transposing, cross_entropy expects B x C x T in batched mode\n",
    "        # This converts into non batched mode\n",
    "        logits: Long[torch.Tensor, \"batch_dim*context_dim latent_dim\"] = logits.view(B * T, C)\n",
    "        # The above is clearly wrong but will not be checked currently\n",
    "        targets: Long[torch.Tensor, \"batch_dim*context_dim\"] = targets.view(B * T)\n",
    "        # https://agustinus.kristia.de/techblog/2016/12/21/forward-reverse-kl/\n",
    "        loss: Float[torch.Tensor, \"\"] = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "def forward_broken(idx: Long[torch.Tensor, \"batch_dim context_dim\"],\n",
    "            targets: Long[torch.Tensor, \"batch_dim context_dim\"]) -> Tuple[\n",
    "    Float[torch.Tensor, \"batch_dim context_dim latent_dim\"], Float[torch.Tensor, \"\"]]:\n",
    "    # idx and targets are both (B,T) tensor of integers\n",
    "    logits: Long[torch.Tensor, \"batch_dim context_dim latent_dim\"] = token_embedding_table(\n",
    "        idx)  # (B,T,C=vocab_size)\n",
    "    # Note that here strictly speaking this does not fix batch size explicitly to B\n",
    "    B, T, C = logits.shape  # (B,T,C=vocab_size)\n",
    "    # Just a hack to avoid transposing, cross_entropy expects B x C x T in batched mode\n",
    "    # This converts into non batched mode\n",
    "    logits: Long[torch.Tensor, \"batch_dim*context_dim latent_dim\"] = logits.view(B * T, C)\n",
    "    # The above is clearly wrong but will not be checked currently\n",
    "    targets: Long[torch.Tensor, \"batch_dim*context_dim\"] = targets.view(B * T)\n",
    "    # https://agustinus.kristia.de/techblog/2016/12/21/forward-reverse-kl/\n",
    "    loss: Float[torch.Tensor, \"\"] = F.cross_entropy(logits, targets)\n",
    "    return logits, loss\n",
    "\n",
    "\n",
    "idx = torch.randint(low=0, high=65, size=(256, 65))\n",
    "targets = torch.randint(low=0, high=65, size=(256, 65))\n",
    "logits_not_none, loss_not_none = forward(idx, targets)\n",
    "logits_none, loss_none = forward(idx, None)\n",
    "logits_broken, loss_broken = forward_broken(idx, targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
