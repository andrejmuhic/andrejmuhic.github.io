[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "andrejmuhic.github.io",
    "section": "",
    "text": "GPT from scratch with comments\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMay 24, 2024\n\n\nAndrej Muhic\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMay 24, 2024\n\n\nAndrej Muhic\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMay 24, 2024\n\n\nAndrej Muhic\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/24_05_2024_gpt2_from_scratch/gpt_dev.html",
    "href": "posts/24_05_2024_gpt2_from_scratch/gpt_dev.html",
    "title": "GPT from scratch with comments",
    "section": "",
    "text": "All credit to Companion notebook to the Zero To Hero video on GPT by Andrej Karpathy.\n\n\n\nAdam/AdamW optimizer\nTransformers\nResidual connections\nDropout\nLayer/batch normalization\nAutomatic differentiation improvements\nPolished libraries that are easy to use, https://pytorch.org/\nHardware acceleration, cuda, tpus, …\n\n\n\n\n\nhttps://kidger.site/thoughts/jaxtyping/\nhttps://docs.kidger.site/jaxtyping/api/array/\nhttps://github.com/patrick-kidger/jaxtyping\nhttps://github.com/agronholm/typeguard\n\n\n\n\n\nhttps://github.com/aimhubio/aim free\nhttps://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html free\nhttps://wandb.ai/site paid\n\n\n\n\n\nhttps://www.ruder.io/optimizing-gradient-descent/#adam\nhttps://iclr-blogposts.github.io/2023/blog/2023/adamw as proximal operator\nhttps://arxiv.org/abs/2404.04454 Implicit Bias of AdamW: ℓ∞ Norm Constrained Optimization optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate) Adam is just using exponential weighted average of gradient and “variance of gradients”, corrected for bias for starting at 0. AdamW adds a trick to do better incorporate L2 regularization for decay rate, best views as proximal operator in my opinion.\n\n\n\n\nMemory constraints even for Adam(W) we need to store two additional vectors thus we 3x our memory requirements.\nBack stepping is not feasible for deep learning, too costly to store harder to parallelize!\nhttps://en.wikipedia.org/wiki/Limited-memory_BFGS approximates inverse of Hessian implicitly and even this is too costly.\nhttps://en.wikipedia.org/wiki/Wolfe_conditions\nhttps://en.wikipedia.org/wiki/Backtracking_line_search more advanced step size\n\n\n\n\n\n\nMy personal favourite: https://github.com/mert-kurttutan/torchview nicer graph, device=‘meta’ is buggy sometimes, use cuda or cpu.\nHonourable mentions: https://github.com/szagoruyko/pytorchviz\n\n\n\n\n\nhttps://www.ai-contentlab.com/2023/03/swishglu-activation-function.html\nhttps://pytorch.org/docs/stable/generated/torch.nn.SiLU.html\n\n\n\n\n\n\n\nhttps://einops.rocks/api/einsum\n\n\n\n\n\nhttps://numpy.org/doc/stable/user/basics.broadcasting.html My advice would be if you are not sure do manual broadcasting and test.\nStrictly test dimenstions with something like jaxtyping\n\n\n\n\n\nInspect your data before you start training\nOverift on small piece of data using simplest possible model before trying to do something fancy\nGetting better and more data will bring larger improvements than using superior model\n\n\n\n\n\n\n\n\nIn practice one would use something like Byte Pair Encoding or WordPiece, SentencePiece ## Attention and geometric deep learning\nAttention: https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention\nGNN and Geometric Deep Learning\nGeometric Deep learning excites me as mathematician as it is a very nice way of infusing domain priors, symmetry, geometry of the problem\n\nTHe full blown image of the multi head attention with learnable positional encoding. Code follows after. It is based on Karpathy notebook with my scratchpad like notes to facilitate deeper understanding. \n\n# Let us do some stricter typing\n%config Completer.use_jedi = False\n# from sklearn.model_selection import train_test_split\nimport typing\nfrom typing import TYPE_CHECKING, Any, Optional\nimport torch\n\n# Strict run time dimension checking\n# https://kidger.site/thoughts/jaxtyping/\n# https://docs.kidger.site/jaxtyping/api/array/\n# https://github.com/patrick-kidger/jaxtyping\n# https://github.com/agronholm/typeguard\n\n# Tracking the progress: \n#                        https://github.com/aimhubio/aim\n#                        https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html\n#                        https://wandb.ai/site\n\n# Something on order of distributions in cross entropy and KL\n# https://agustinus.kristia.de/techblog/2016/12/21/forward-reverse-kl/\n\n# Visualizing the network\n# https://github.com/szagoruyko/pytorchviz\n# https://github.com/mert-kurttutan/torchview nicer graph\n\n# Optimization, AdamW state of the art\n# https://www.ruder.io/optimizing-gradient-descent/#adam\n# https://iclr-blogposts.github.io/2023/blog/2023/adamw/ as proximal operator\n# https://arxiv.org/abs/2404.04454 Implicit Bias of AdamW: ℓ∞ Norm Constrained Optimization\n# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n# exponential weighted average of gradient and \"variance of gradients\", corrected for bias for starting at 0 \n# +  a trick to do better than L2 for decay rate\n# Back stepping\n# Not feasible for deep learning, too costly to store harder to parallelize!\n# https://en.wikipedia.org/wiki/Limited-memory_BFGS approximates inverse of Hessian implicitly and even this is too costly\n# https://en.wikipedia.org/wiki/Wolfe_conditions\n# https://en.wikipedia.org/wiki/Backtracking_line_search more advanced step size\n\n# Activations\n# https://www.ai-contentlab.com/2023/03/swishglu-activation-function.html\n# https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html\n\n# Einsum formulas for human, thanks for the hint guys!\n# https://einops.rocks/api/einsum/\n\n# Broadcasting\n# https://numpy.org/doc/stable/user/basics.broadcasting.html My advice would be if you are not sure do manual broadcasting and test.\n# Strictly test dimenstions with something like jaxtyping\n\n# key enabling techiques that made deep learning more easy to work with\n# - adam/adamw optimizer\n# - residual connections\n# - dropout\n# - layer/batch normalization\n# - automatic differentiation\n\nfrom jaxtyping import Float, Int64\n\nArray: typing.TypeAlias = torch.Tensor\nLong: typing.TypeAlias = Int64\n\n\n# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n\n--2024-04-24 11:09:39--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: ‘input.txt.23’\n\ninput.txt.23        100%[===================&gt;]   1.06M  --.-KB/s    in 0.07s   \n\n2024-04-24 11:09:39 (15.7 MB/s) - ‘input.txt.23’ saved [1115394/1115394]\n\n\n\n\n# read it in to inspect it\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n\nprint(\"length of dataset in characters: \", len(text))\n\nlength of dataset in characters:  1115394\n\n\n\n# let's look at the first 1000 characters\nprint(text[:1000])\n\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge.\n\n\n\n\n\n# here are all the unique characters that occur in this text\nchars: list[str] = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)\n\n\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n\n\n\n# create a mapping from characters to integers\nstoi: dict[str, int] = {ch: i for i, ch in enumerate(chars)}\nitos: dict[int, str] = {i: ch for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n\n# In practice one would use something like [Byte Pair Encoding](https://github.com/karpathy/minbpe) or [WordPiece, SentencePiece](https://huggingface.co/docs/transformers/en/tokenizer_summary)\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n\n[46, 47, 47, 1, 58, 46, 43, 56, 43]\nhii there\n\n\n\n# let's now encode the entire text dataset and store it into a torch.Tensor\nimport torch  # we use PyTorch: https://pytorch.org\n\ndata = torch.tensor(encode(text), dtype=torch.int64)\nprint(data.shape, data.dtype)\nprint(data[:1000])  # the 1000 characters we looked at earier will to the GPT look like this\n\ntorch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n\n\n\n# Let's now split up the data into train and validation sets\nn = int(0.9 * len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n\nblock_size = 8\n# First block and the target\ntrain_data[:block_size + 1]\n\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n\n\n\n# This is what we want to learn for max_context_size = block_size\n# It is important that we train also for shorter sequences than block_size\n# When creating batches in practice we could need padding token and if pad left or right!\n# Also how to shuffle if we need to if data does not fit in memory: https://blog.janestreet.com/how-to-shuffle-a-big-dataset/\nx = train_data[:block_size]\ny = train_data[1:block_size + 1]\nfor t in range(block_size):\n    context = x[:t + 1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n\nwhen input is tensor([18]) the target: 47\nwhen input is tensor([18, 47]) the target: 56\nwhen input is tensor([18, 47, 56]) the target: 57\nwhen input is tensor([18, 47, 56, 57]) the target: 58\nwhen input is tensor([18, 47, 56, 57, 58]) the target: 1\nwhen input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n\n\n\ntorch.manual_seed(1337)\nbatch_size = 4  # how many independent sequences will we process in parallel?\nblock_size = 8  # what is the maximum context length for predictions?\n\n\ndef get_batch(split_kind: str) -&gt; tuple[Int64[Array, \"n_batches block_size\"], Int64[Array, \"n_batches block_size\"]]:\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split_kind == 'train' else val_data\n    # Random starting indices of blocks, notice that blocks can overlap\n    # To do something like this for real: \n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i + block_size] for i in ix])\n    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n    return x, y\n\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('----')\n\nfor b in range(batch_size):  # batch dimension\n    for t in range(block_size):  # time dimension\n        context = xb[b, :t + 1]\n        target = yb[b, t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n\ninputs:\ntorch.Size([4, 8])\ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\ntargets:\ntorch.Size([4, 8])\ntensor([[43, 58,  5, 57,  1, 46, 43, 39],\n        [53, 56,  1, 58, 46, 39, 58,  1],\n        [58,  1, 58, 46, 39, 58,  1, 46],\n        [17, 27, 10,  0, 21,  1, 54, 39]])\n----\nwhen input is [24] the target: 43\nwhen input is [24, 43] the target: 58\nwhen input is [24, 43, 58] the target: 5\nwhen input is [24, 43, 58, 5] the target: 57\nwhen input is [24, 43, 58, 5, 57] the target: 1\nwhen input is [24, 43, 58, 5, 57, 1] the target: 46\nwhen input is [24, 43, 58, 5, 57, 1, 46] the target: 43\nwhen input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\nwhen input is [44] the target: 53\nwhen input is [44, 53] the target: 56\nwhen input is [44, 53, 56] the target: 1\nwhen input is [44, 53, 56, 1] the target: 58\nwhen input is [44, 53, 56, 1, 58] the target: 46\nwhen input is [44, 53, 56, 1, 58, 46] the target: 39\nwhen input is [44, 53, 56, 1, 58, 46, 39] the target: 58\nwhen input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52] the target: 58\nwhen input is [52, 58] the target: 1\nwhen input is [52, 58, 1] the target: 58\nwhen input is [52, 58, 1, 58] the target: 46\nwhen input is [52, 58, 1, 58, 46] the target: 39\nwhen input is [52, 58, 1, 58, 46, 39] the target: 58\nwhen input is [52, 58, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\nwhen input is [25] the target: 17\nwhen input is [25, 17] the target: 27\nwhen input is [25, 17, 27] the target: 10\nwhen input is [25, 17, 27, 10] the target: 0\nwhen input is [25, 17, 27, 10, 0] the target: 21\nwhen input is [25, 17, 27, 10, 0, 21] the target: 1\nwhen input is [25, 17, 27, 10, 0, 21, 1] the target: 54\nwhen input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n\n\n\nprint(xb)  # our input to the transformer\n\ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\n\n\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\ntorch.manual_seed(1337)\n\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size: int):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx: Long[torch.Tensor, \"batch_dim context_dim\"],\n                targets: Optional[Long[torch.Tensor, \"batch_dim context_dim\"]] = None):\n\n        # idx and targets are both (B,T) tensor of integers\n        logits: Long[torch.Tensor, \"batch_dim context_dim latent_dim\"] = self.token_embedding_table(\n            idx)  # (B,T,C=vocab_size)\n\n        if targets is None:\n            return logits, None\n        else:\n            # Note that here strictly speaking this does not fix batch size explicitly to B\n            B, T, C = logits.shape  # (B,T,C=vocab_size)\n            # Just a hack to avoid transposing, cross_entropy expects B x C x T in batched mode\n            # This converts into non batched mode\n            logits: Long[torch.Tensor, \"batch_dim*context_dim latent_dim\"] = logits.view(B * T, C)\n            targets: Long[torch.Tensor, \"batch_dim*context_dim\"] = targets.view(B * T)\n            # https://agustinus.kristia.de/techblog/2016/12/21/forward-reverse-kl/\n            loss: Float[torch.Tensor, \"\"] = F.cross_entropy(logits, targets)\n            return logits, loss\n\n    def generate(self, idx: Long[torch.Tensor, \"batch_dim context_dim\"], max_new_tokens: int):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)\n            # focus only on the last time step\n            logits = logits[:, -1, :]  # becomes (B, vocab_size)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1)  # (B, vocab_size)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n            # What can go wrong here? and it is not handled at all\n        return idx\n\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\nprint(loss.shape)\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n\ntorch.Size([32, 65])\ntensor(4.8786, grad_fn=&lt;NllLossBackward0&gt;)\ntorch.Size([])\n\nSr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n\n\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n# weighted average of mean and \"variance\" of gradients +  a trick\n# Not feasible for deep learning, too costly to store harder to parallelize!\n# https://en.wikipedia.org/wiki/Limited-memory_BFGS approximates inverse of Hessian implicitly and even this is too costly\n# https://en.wikipedia.org/wiki/Wolfe_conditions\n# https://en.wikipedia.org/wiki/Backtracking_line_search more advanced step size\n\n\nbatch_size = 32\nn_steps = 1_000\nfor steps in range(n_steps):  # increase number of steps for good results...\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n\n3.7218432426452637\n\n\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n\n\nolylvLLko'TMyatyIoconxad.?-tNSqYPsx&bF.oiR;BD$dZBMZv'K f bRSmIKptRPly:AUC&$zLK,qUEy&Ay;ZxjKVhmrdagC-bTop-QJe.H?x\nJGF&pwst-P sti.hlEsu;w:w a BG:tLhMk,epdhlay'sVzLq--ERwXUzDnq-bn czXxxI&V&Pynnl,s,Ioto!uvixwC-IJXElrgm C-.bcoCPJ\nIMphsevhO AL!-K:AIkpre,\nrPHEJUzV;P?uN3b?ohoRiBUENoV3B&jumNL;Aik,\nxf -IEKROn JSyYWW?n 'ay;:weO'AqVzPyoiBL? seAX3Dot,iy.xyIcf r!!ul-Koi:x pZrAQly'v'a;vEzN\nBwowKo'MBqF$PPFb\nCjYX3beT,lZ qdda!wfgmJP\nDUfNXmnQU mvcv?nlnQF$JUAAywNocd  bGSPyAlprNeQnq-GRSVUP.Ja!IBoDqfI&xJM AXEHV&DKvRS\n\n\n\n\n\n\n\n# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\ntorch.manual_seed(42)\n# Causal attention does not take into account future information\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\nprint('a=')\nprint(a)\nprint('--')\nprint('b=')\nprint(b)\nprint('--')\nprint('c=')\nprint(c)\n\na=\ntensor([[1.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000],\n        [0.3333, 0.3333, 0.3333]])\n--\nb=\ntensor([[2., 7.],\n        [6., 4.],\n        [6., 5.]])\n--\nc=\ntensor([[2.0000, 7.0000],\n        [4.0000, 5.5000],\n        [4.6667, 5.3333]])\n\n\n\n# consider the following toy example:\n\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 2  # batch, time, channels\nx = torch.randn(B, T, C)\nx.shape\n\ntorch.Size([4, 8, 2])\n\n\n\n# We want $x[b,t] = mean_{i&lt;=t} x[b,i]$\nxbow = torch.zeros((B, T, C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b, :t + 1]  # (t,C)\n        xbow[b, t] = torch.mean(xprev, 0)\n\n\n# version 2: using matrix multiply for a weighted aggregation\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxbow2 = wei @ x  # (B, T, T) @ (B, T, C) ----&gt; (B, T, C)\ntorch.allclose(xbow, xbow2)\n\nFalse\n\n\n\n# version 3: use Softmax\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\n# This seems strange at first glance but exp(-inf) = 0 and it is well defined, otherwise training would get broken constantly\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n\nFalse\n\n\n\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---&gt; (B, T, T)\n\ntril = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T,T))\n# Full matrix multiplication is faster, just block what we do not need\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\n\ntorch.Size([4, 8, 16])\n\n\n\nwei[0]\n\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n       grad_fn=&lt;SelectBackward0&gt;)\n\n\nNotes: - Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights. - There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens. - Each example across batch dimension is of course processed completely independently and never “talk” to each other - In an “encoder” attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a “decoder” attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling. - “self-attention” just means that the keys and values are produced from the same source as queries. In “cross-attention”, the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module) - “Scaled” attention additionally divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below\n\n# head_size is the dimension of \"latent space\"\nk = torch.randn(B, T, head_size)\nq = torch.randn(B, T, head_size)\nwei = q @ k.transpose(-2, -1) * head_size ** -0.5\n\n\nk.var()\n\ntensor(1.0449)\n\n\n\nq.var()\n\ntensor(1.0700)\n\n\n\nwei.var()\n\ntensor(1.0918)\n\n\n\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)\n\ntensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n\n\n\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]) * 8, dim=-1)  # gets too peaky, converges to one-hot\n\ntensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n\n\n\nclass LayerNorm1d:  # (used to be BatchNorm1d)\n\n    def __init__(self, dim, eps=1e-5, momentum=0.1):\n        self.eps = eps\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n\n    def __call__(self, x):\n        # calculate the forward pass\n        xmean = x.mean(1, keepdim=True)  # batch mean\n        xvar = x.var(1, keepdim=True)  # batch variance\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n        self.out = self.gamma * xhat + self.beta\n        return self.out\n\n    def parameters(self):\n        return [self.gamma, self.beta]\n\n\ntorch.manual_seed(1337)\nmodule = LayerNorm1d(100)\n# batch_norm_1d = nn.BatchNorm1d(100)\n# x_normalized_torch = batch_norm_1d(x)\n\nx = torch.randn(32, 100)  # batch size 32 of 100-dimensional vectors\nx_normalized = module(x)\nx_normalized.shape\n\ntorch.Size([32, 100])\n\n\n\nx[:, 0].mean(), x[:, 0].std()  # mean,std of one feature across all batch inputs\n\n(tensor(0.1392), tensor(0.8899))\n\n\n\nx[0, :].mean(), x[0, :].std()  # mean,std of a single input from the batch, of its features\n\n(tensor(0.0409), tensor(1.0476))\n\n\n\n# French to English translation example:\n\n# &lt;--------- ENCODE ------------------&gt;&lt;--------------- DECODE -----------------&gt;\n# les réseaux de neurones sont géniaux! &lt;START&gt; neural networks are awesome!&lt;END&gt;\n\n\n\nYou may want to refer directly to the git repo instead though.\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 1024 # 16  # how many independent sequences will we process in parallel?\nblock_size = 128 # 32  # what is the maximum context length for predictions?\nmax_iters = 10000\neval_interval = 100\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 64\nn_head = 4\nn_layer = 4\ndropout = 0.0\n# ------------\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n# create a mapping from characters to integers\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9 * len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i + block_size] for i in ix])\n    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention\n        https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention\n    \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        # This limits us to the maximal context block_size\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)  # (B,T,C=n_embd) -&gt; (B,T,C=head_size)\n        q = self.query(x)  # (B,T,C=n_embd) -&gt; (B,T,C=head_size)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2, -1) * C ** -0.5  # (B, T, C=head_size) @ (B, C=head_size), T) -&gt; (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n        # The drop out is over full matrix, alternatively it would be better to just drop on mask, this is biased\n        # Also it seems maybe conceptually we should just do symmetric dropout\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x)  # (B,T,C=head_size)\n        # The matrix multiplication is batched and applied on last two dimensions!\n        out = wei @ v  # (B, T, T) @ (B, T, C=head_size) -&gt; (B, T, C=head_size)\n        return out\n\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        # Projection back to residual pathway, align the basis\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd), # Projection back to residual pathway\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd: int, n_head: int):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        # To guarantee that final concatenated embedding is of size n_embd \n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        # Skip connections added to improve the flow of gradient\n        # Need to project back to residual pathway in sa and ffwd to \"align bases\" \n        \n        # Modern way is do to layer norm before and not after, in original paper it was done after, we do before!\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\n\n# Not so super simple and not bigram model anymore\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token is mapped to latent space of size n_embd\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        # learnable position embedding for positions 0, ..., block_size - 1\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        # Modern way of handling this is:\n        # https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/\n        # https://afterhoursresearch.hashnode.dev/rope-rotary-positional-embedding\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx)  # (B,T,C=n_embd)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C=n_embd)\n        x = tok_emb + pos_emb  # (B,T,C=n_embd)\n        x = self.blocks(x)  # (B,T,C=n_embd)\n        # Modern way is do to layer norm before and not after, in original paper it was done after, we do before!\n        x = self.ln_f(x)  # (B,T,C=n_embd)\n        logits = self.lm_head(x)  # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B * T, C)\n            targets = targets.view(B * T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens, otherwise things will explode\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :]  # (B, T, C=vocab_size) becomes (B, C=vocab_size)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1)  # (B, C=vocab_size)\n            # sample from the multinomial distribution\n            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n        return idx\n\n\nmodel = BigramLanguageModel()\nm = model.to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in m.parameters()) / 1e6, 'M parameters')\n\n# create a PyTorch optimizer\n# https://www.ruder.io/optimizing-gradient-descent/#adam\n# https://iclr-blogposts.github.io/2023/blog/2023/adamw/\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n# Temperature motivation https://en.wikipedia.org/wiki/LogSumExp\n# What happens when you divide logits by posite Temp and then do softmax\n# Temp close to 0 is max in limit, one hot, Temp &gt;&gt; 1 in limit random\n\n0.215873 M parameters\nstep 0: train loss 4.3254, val loss 4.3198\nstep 100: train loss 2.5600, val loss 2.5620\nstep 200: train loss 2.4232, val loss 2.4276\nstep 300: train loss 2.2731, val loss 2.2882\nstep 400: train loss 2.1599, val loss 2.1881\nstep 500: train loss 2.0462, val loss 2.0977\nstep 600: train loss 1.9539, val loss 2.0254\nstep 700: train loss 1.8708, val loss 1.9669\nstep 800: train loss 1.8075, val loss 1.9265\nstep 900: train loss 1.7639, val loss 1.8996\nstep 1000: train loss 1.7138, val loss 1.8639\nstep 1100: train loss 1.6799, val loss 1.8366\nstep 1200: train loss 1.6495, val loss 1.8171\nstep 1300: train loss 1.6253, val loss 1.7901\nstep 1400: train loss 1.6056, val loss 1.7831\nstep 1500: train loss 1.5937, val loss 1.7649\nstep 1600: train loss 1.5675, val loss 1.7471\nstep 1700: train loss 1.5596, val loss 1.7430\nstep 1800: train loss 1.5432, val loss 1.7324\nstep 1900: train loss 1.5295, val loss 1.7198\nstep 2000: train loss 1.5209, val loss 1.7141\nstep 2100: train loss 1.5125, val loss 1.7100\nstep 2200: train loss 1.5057, val loss 1.7036\nstep 2300: train loss 1.4972, val loss 1.6989\nstep 2400: train loss 1.5017, val loss 1.7052\nstep 2500: train loss 1.4788, val loss 1.6915\nstep 2600: train loss 1.4783, val loss 1.6897\nstep 2700: train loss 1.4680, val loss 1.6796\nstep 2800: train loss 1.4690, val loss 1.6847\nstep 2900: train loss 1.4657, val loss 1.6848\nstep 3000: train loss 1.4583, val loss 1.6755\nstep 3100: train loss 1.4511, val loss 1.6711\nstep 3200: train loss 1.4423, val loss 1.6699\nstep 3300: train loss 1.4397, val loss 1.6653\nstep 3400: train loss 1.4379, val loss 1.6670\nstep 3500: train loss 1.4387, val loss 1.6658\nstep 3600: train loss 1.4324, val loss 1.6644\nstep 3700: train loss 1.4373, val loss 1.6753\nstep 3800: train loss 1.4368, val loss 1.6735\nstep 3900: train loss 1.4280, val loss 1.6606\nstep 4000: train loss 1.4182, val loss 1.6546\nstep 4100: train loss 1.4152, val loss 1.6576\nstep 4200: train loss 1.4103, val loss 1.6493\nstep 4300: train loss 1.4107, val loss 1.6541\nstep 4400: train loss 1.4070, val loss 1.6480\nstep 4500: train loss 1.4079, val loss 1.6546\nstep 4600: train loss 1.4007, val loss 1.6493\nstep 4700: train loss 1.4011, val loss 1.6473\nstep 4800: train loss 1.3991, val loss 1.6462\nstep 4900: train loss 1.3944, val loss 1.6454\nstep 5000: train loss 1.3910, val loss 1.6423\nstep 5100: train loss 1.3937, val loss 1.6460\nstep 5200: train loss 1.3922, val loss 1.6466\nstep 5300: train loss 1.3891, val loss 1.6476\nstep 5400: train loss 1.3853, val loss 1.6426\nstep 5500: train loss 1.3806, val loss 1.6390\nstep 5600: train loss 1.3917, val loss 1.6519\nstep 5700: train loss 1.3813, val loss 1.6436\nstep 5800: train loss 1.3840, val loss 1.6437\nstep 5900: train loss 1.3801, val loss 1.6430\nstep 6000: train loss 1.3750, val loss 1.6401\nstep 6100: train loss 1.3709, val loss 1.6355\nstep 6200: train loss 1.3721, val loss 1.6365\nstep 6300: train loss 1.3685, val loss 1.6341\nstep 6400: train loss 1.3709, val loss 1.6338\nstep 6500: train loss 1.3636, val loss 1.6296\nstep 6600: train loss 1.3659, val loss 1.6315\nstep 6700: train loss 1.3622, val loss 1.6283\nstep 6800: train loss 1.3635, val loss 1.6318\nstep 6900: train loss 1.3647, val loss 1.6374\nstep 7000: train loss 1.3653, val loss 1.6358\nstep 7100: train loss 1.3614, val loss 1.6366\nstep 7200: train loss 1.3581, val loss 1.6324\nstep 7300: train loss 1.3528, val loss 1.6317\nstep 7400: train loss 1.3544, val loss 1.6292\nstep 7500: train loss 1.3523, val loss 1.6288\nstep 7600: train loss 1.3500, val loss 1.6294\nstep 7700: train loss 1.3504, val loss 1.6278\nstep 7800: train loss 1.3503, val loss 1.6281\nstep 7900: train loss 1.3431, val loss 1.6207\nstep 8000: train loss 1.3466, val loss 1.6252\nstep 8100: train loss 1.3486, val loss 1.6304\nstep 8200: train loss 1.3413, val loss 1.6257\nstep 8300: train loss 1.3434, val loss 1.6233\nstep 8400: train loss 1.3417, val loss 1.6285\nstep 8500: train loss 1.3397, val loss 1.6190\nstep 8600: train loss 1.3404, val loss 1.6283\nstep 8700: train loss 1.3382, val loss 1.6265\nstep 8800: train loss 1.3364, val loss 1.6270\nstep 8900: train loss 1.3345, val loss 1.6193\nstep 9000: train loss 1.3342, val loss 1.6248\nstep 9100: train loss 1.3363, val loss 1.6275\nstep 9200: train loss 1.3332, val loss 1.6228\nstep 9300: train loss 1.3345, val loss 1.6238\nstep 9400: train loss 1.3334, val loss 1.6260\nstep 9500: train loss 1.3359, val loss 1.6290\nstep 9600: train loss 1.3313, val loss 1.6245\nstep 9700: train loss 1.3268, val loss 1.6246\nstep 9800: train loss 1.3321, val loss 1.6256\nstep 9900: train loss 1.3272, val loss 1.6196\nstep 9999: train loss 1.3266, val loss 1.6243\n\n\nYORK:\nI beggan in This name King Peter, and envy\ngrades the change art that us he had beck:\nAs was, away, my facts arrizons heavens,\nHow is he cover heldly, and if the neck. Take my lieve to the\nWill may in wabgunel lind tear.\n\nAUFIDIUS:\nWhat! O have plaw you:'ld me not\nTo tell I will the shamed again Willohed their king.\n\nGRUMIO:\nEven his no the comes for under danger,\nWhich arms you must with all on,\nTo fight? grant nor furscience you acharding again,\nFor hither to mark no Rutland, and for his needy out\nIs yourselves gatesman; a they arms? what the men.\nWas mayst thou sworn to the dead of cour speaks\nFor mighty my castle lack and sorted and hellow.\nLading to Leond more over Heifts, upon thou fest.\nAs not controves, she shall look to entrough.\n\nKING HENRY VI:\nHeaving with kill it bump whose told lady.\n\nProvost:\nHe had abhold. I know the no none Captuluse: I do it is\nouthfunance eyes and approach'd when no mother.\n\nGLOUCESTER:\nHoour lord, why, because I, thus wounder for Gloucester.\nYet I am a runse. This I husband; go whil though,\nLook not take all the went should men,\nTrue thinks and endly in it infected greates,\nBut now, call King Henry's fatnerment their\nOf prayer be any inkering lament tell thee expection not myself.\n\nBUCKINGHAM:\nThe time for Hereford!\nThen lands! then often foolish'd her wantsrified?\n\nSecond Kench English clock?\n\nKING EDWARD IV:\nMy love the begun,\nGood queen the to the lave's else of my lorious numblemen'd,\nThe people, age finger I attend are holess!\n\nROMEO:\nIf they shut be the shall, as I tho\nmatter virtuce!\nCountry thee till thee world, she worse, who drink?\n\nMARCIUS:\nLet fortune.\n\nBENVOLIO:\nI am wound are to-marriage,\nBeath, could hither of some to, my hearth; can\nhard I'ld not father.\n\nLUCIO:\nI have power, Hastings Lanca.\n\nPRINCE:\nOur whiptee, before a gentle!\n\nCLARENCE:\n'Fitter like he punish'd, go to the autch'd:\nAnd fancy you, yet it an any any sent,\nBut battle comfort, Juliet pardon.\n\nISABELLA:\nO'I fight he to the good the world and u\n\n\n\n# https://github.com/szagoruyko/pytorchviz\n# https://github.com/mert-kurttutan/torchview nicer graph\n# https://github.com/mert-kurttutan/torchview\nfrom torchview import draw_graph\nimport torchlens as tl\n\n# device='meta' -&gt; no memory is consumed for visualization\nprint(xb.shape)\nprint(yb.shape)\nmodel_graph = draw_graph(model, input_data=(xb, yb), device='cuda')\nmodel_graph.visual_graph\n\n# model_history = tl.log_forward_pass(model, (xb, yb)\n#                                      , layers_to_save='all', vis_opt='unrolled')\n# print(model_history)\n\nNameError: name 'model_history' is not defined\n\n\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(model.generate(context, max_new_tokens=2000)[0].tolist()))\n\n\nCity and by his warms, went's scomptude, that\ntill ragge untong-make's.' But, what a determy, thou\nfaintors, you attend underpt my mother.\n\nSecond Men, an well.\nCome I mean as thing once with I recond this;\nAnd I may of looks to a plenity,\nAnd fears 'free and joy myself,\nWhen gazes liege titles wise and my restraint.\n\nKING EDWARD IV:\nWhoel, she was this head.\n\nDUKE OF YORK:\nThen frame thyself? for he forth and in this.\nWere commend in the fiends, o' the voice\nMy mother counsished, to two Ely; the counter\nThat I do. So an ever maid in beast plots.\n\nDUKE O FRIZWIIC John:\nUpon thine is kind; too, Tower; thou dust baldgiant\nAnd dost happille to the poore's doin work her,\nMarriabe.\n\nBENVOLIO:\nIf ends ye.\n\nSICINIUS:\nGood love I had unto conveys,\nTo my sovereign i' the dove me.\n\nHENRY BOLINGHAM:\nWere glast you shall be as the cannot both:\nThou art me in my consent until thee,\nOr thee to direct against partiest;\nBut! seeing she parks of cross on me age good\nSo queen will answering doth chase, letter.\nSo, that you on my lady.\n\nDUKE OF AUMERLE:\nNo fellow and Romeo Edward, willingly of God,\nAnd Bonding when the sistern he widow;\nFor hung else hour tope head up this converses!\nAway she, who is, I'll same of brought,\nArt should, that now their father?\n\nPOMPEY:\nSicile you will: have you were not speak to the blot,\nDisself that winder my lady me.\nand loya 'twas your field this fathes,\nI shall my fortune joys and I cit thou are;\nThen wounds speak them not speak.\nIf God, my lord, as it is this many coundation,\nFor and crowned the acce.\n\nMENENIUS:\nWay, pardon, his patience;\nWill give and we may bring you disdain's basters.\nYou threw attailymy. Go traitor were.\n\nSARMIANA:\nNot before; mannish wife?\n\nCOMINIUS:\nLift--\n\nGLOUCESTER:\nWhy villain again, I'll make you misber in tent\nRedly, listeen heath will his us were of senate\nIs no amient look'd yet a little.\n\nMENENIUS:\nI did his body throwed of this?\n\nPage:\nMy womb, Edward house will and deserve it.\n\nLEONTES:\nIt you it do.\n\nDUKE VINCENTI"
  },
  {
    "objectID": "posts/24_05_2024_gpt2_from_scratch/gpt_dev.html#key-techiques-so-deep-learning-is-easier-to-use",
    "href": "posts/24_05_2024_gpt2_from_scratch/gpt_dev.html#key-techiques-so-deep-learning-is-easier-to-use",
    "title": "GPT from scratch with comments",
    "section": "",
    "text": "Adam/AdamW optimizer\nTransformers\nResidual connections\nDropout\nLayer/batch normalization\nAutomatic differentiation improvements\nPolished libraries that are easy to use, https://pytorch.org/\nHardware acceleration, cuda, tpus, …"
  },
  {
    "objectID": "posts/24_05_2024_gpt2_from_scratch/gpt_dev.html#strict-run-time-named-dimension-checking",
    "href": "posts/24_05_2024_gpt2_from_scratch/gpt_dev.html#strict-run-time-named-dimension-checking",
    "title": "GPT from scratch with comments",
    "section": "",
    "text": "https://kidger.site/thoughts/jaxtyping/\nhttps://docs.kidger.site/jaxtyping/api/array/\nhttps://github.com/patrick-kidger/jaxtyping\nhttps://github.com/agronholm/typeguard"
  },
  {
    "objectID": "posts/24_05_2024_gpt2_from_scratch/gpt_dev.html#tracking-progress-of-your-runs",
    "href": "posts/24_05_2024_gpt2_from_scratch/gpt_dev.html#tracking-progress-of-your-runs",
    "title": "GPT from scratch with comments",
    "section": "",
    "text": "https://github.com/aimhubio/aim free\nhttps://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html free\nhttps://wandb.ai/site paid"
  },
  {
    "objectID": "posts/24_05_2024_gpt2_from_scratch/gpt_dev.html#optimization-adamw-state-of-the-art",
    "href": "posts/24_05_2024_gpt2_from_scratch/gpt_dev.html#optimization-adamw-state-of-the-art",
    "title": "GPT from scratch with comments",
    "section": "",
    "text": "https://www.ruder.io/optimizing-gradient-descent/#adam\nhttps://iclr-blogposts.github.io/2023/blog/2023/adamw as proximal operator\nhttps://arxiv.org/abs/2404.04454 Implicit Bias of AdamW: ℓ∞ Norm Constrained Optimization optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate) Adam is just using exponential weighted average of gradient and “variance of gradients”, corrected for bias for starting at 0. AdamW adds a trick to do better incorporate L2 regularization for decay rate, best views as proximal operator in my opinion.\n\n\n\n\nMemory constraints even for Adam(W) we need to store two additional vectors thus we 3x our memory requirements.\nBack stepping is not feasible for deep learning, too costly to store harder to parallelize!\nhttps://en.wikipedia.org/wiki/Limited-memory_BFGS approximates inverse of Hessian implicitly and even this is too costly.\nhttps://en.wikipedia.org/wiki/Wolfe_conditions\nhttps://en.wikipedia.org/wiki/Backtracking_line_search more advanced step size"
  },
  {
    "objectID": "posts/24_05_2024_gpt2_from_scratch/gpt_dev.html#visualizing-the-network",
    "href": "posts/24_05_2024_gpt2_from_scratch/gpt_dev.html#visualizing-the-network",
    "title": "GPT from scratch with comments",
    "section": "",
    "text": "My personal favourite: https://github.com/mert-kurttutan/torchview nicer graph, device=‘meta’ is buggy sometimes, use cuda or cpu.\nHonourable mentions: https://github.com/szagoruyko/pytorchviz"
  },
  {
    "objectID": "posts/24_05_2024_gpt2_from_scratch/gpt_dev.html#activations",
    "href": "posts/24_05_2024_gpt2_from_scratch/gpt_dev.html#activations",
    "title": "GPT from scratch with comments",
    "section": "",
    "text": "https://www.ai-contentlab.com/2023/03/swishglu-activation-function.html\nhttps://pytorch.org/docs/stable/generated/torch.nn.SiLU.html"
  },
  {
    "objectID": "posts/24_05_2024_gpt2_from_scratch/gpt_dev.html#useful-tricks",
    "href": "posts/24_05_2024_gpt2_from_scratch/gpt_dev.html#useful-tricks",
    "title": "GPT from scratch with comments",
    "section": "",
    "text": "https://einops.rocks/api/einsum\n\n\n\n\n\nhttps://numpy.org/doc/stable/user/basics.broadcasting.html My advice would be if you are not sure do manual broadcasting and test.\nStrictly test dimenstions with something like jaxtyping\n\n\n\n\n\nInspect your data before you start training\nOverift on small piece of data using simplest possible model before trying to do something fancy\nGetting better and more data will bring larger improvements than using superior model"
  },
  {
    "objectID": "posts/24_05_2024_gpt2_from_scratch/gpt_dev.html#nlp-specific-info",
    "href": "posts/24_05_2024_gpt2_from_scratch/gpt_dev.html#nlp-specific-info",
    "title": "GPT from scratch with comments",
    "section": "",
    "text": "In practice one would use something like Byte Pair Encoding or WordPiece, SentencePiece ## Attention and geometric deep learning\nAttention: https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention\nGNN and Geometric Deep Learning\nGeometric Deep learning excites me as mathematician as it is a very nice way of infusing domain priors, symmetry, geometry of the problem\n\nTHe full blown image of the multi head attention with learnable positional encoding. Code follows after. It is based on Karpathy notebook with my scratchpad like notes to facilitate deeper understanding. \n\n# Let us do some stricter typing\n%config Completer.use_jedi = False\n# from sklearn.model_selection import train_test_split\nimport typing\nfrom typing import TYPE_CHECKING, Any, Optional\nimport torch\n\n# Strict run time dimension checking\n# https://kidger.site/thoughts/jaxtyping/\n# https://docs.kidger.site/jaxtyping/api/array/\n# https://github.com/patrick-kidger/jaxtyping\n# https://github.com/agronholm/typeguard\n\n# Tracking the progress: \n#                        https://github.com/aimhubio/aim\n#                        https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html\n#                        https://wandb.ai/site\n\n# Something on order of distributions in cross entropy and KL\n# https://agustinus.kristia.de/techblog/2016/12/21/forward-reverse-kl/\n\n# Visualizing the network\n# https://github.com/szagoruyko/pytorchviz\n# https://github.com/mert-kurttutan/torchview nicer graph\n\n# Optimization, AdamW state of the art\n# https://www.ruder.io/optimizing-gradient-descent/#adam\n# https://iclr-blogposts.github.io/2023/blog/2023/adamw/ as proximal operator\n# https://arxiv.org/abs/2404.04454 Implicit Bias of AdamW: ℓ∞ Norm Constrained Optimization\n# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n# exponential weighted average of gradient and \"variance of gradients\", corrected for bias for starting at 0 \n# +  a trick to do better than L2 for decay rate\n# Back stepping\n# Not feasible for deep learning, too costly to store harder to parallelize!\n# https://en.wikipedia.org/wiki/Limited-memory_BFGS approximates inverse of Hessian implicitly and even this is too costly\n# https://en.wikipedia.org/wiki/Wolfe_conditions\n# https://en.wikipedia.org/wiki/Backtracking_line_search more advanced step size\n\n# Activations\n# https://www.ai-contentlab.com/2023/03/swishglu-activation-function.html\n# https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html\n\n# Einsum formulas for human, thanks for the hint guys!\n# https://einops.rocks/api/einsum/\n\n# Broadcasting\n# https://numpy.org/doc/stable/user/basics.broadcasting.html My advice would be if you are not sure do manual broadcasting and test.\n# Strictly test dimenstions with something like jaxtyping\n\n# key enabling techiques that made deep learning more easy to work with\n# - adam/adamw optimizer\n# - residual connections\n# - dropout\n# - layer/batch normalization\n# - automatic differentiation\n\nfrom jaxtyping import Float, Int64\n\nArray: typing.TypeAlias = torch.Tensor\nLong: typing.TypeAlias = Int64\n\n\n# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n\n--2024-04-24 11:09:39--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: ‘input.txt.23’\n\ninput.txt.23        100%[===================&gt;]   1.06M  --.-KB/s    in 0.07s   \n\n2024-04-24 11:09:39 (15.7 MB/s) - ‘input.txt.23’ saved [1115394/1115394]\n\n\n\n\n# read it in to inspect it\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n\nprint(\"length of dataset in characters: \", len(text))\n\nlength of dataset in characters:  1115394\n\n\n\n# let's look at the first 1000 characters\nprint(text[:1000])\n\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge.\n\n\n\n\n\n# here are all the unique characters that occur in this text\nchars: list[str] = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)\n\n\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n\n\n\n# create a mapping from characters to integers\nstoi: dict[str, int] = {ch: i for i, ch in enumerate(chars)}\nitos: dict[int, str] = {i: ch for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n\n# In practice one would use something like [Byte Pair Encoding](https://github.com/karpathy/minbpe) or [WordPiece, SentencePiece](https://huggingface.co/docs/transformers/en/tokenizer_summary)\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n\n[46, 47, 47, 1, 58, 46, 43, 56, 43]\nhii there\n\n\n\n# let's now encode the entire text dataset and store it into a torch.Tensor\nimport torch  # we use PyTorch: https://pytorch.org\n\ndata = torch.tensor(encode(text), dtype=torch.int64)\nprint(data.shape, data.dtype)\nprint(data[:1000])  # the 1000 characters we looked at earier will to the GPT look like this\n\ntorch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n\n\n\n# Let's now split up the data into train and validation sets\nn = int(0.9 * len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n\nblock_size = 8\n# First block and the target\ntrain_data[:block_size + 1]\n\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n\n\n\n# This is what we want to learn for max_context_size = block_size\n# It is important that we train also for shorter sequences than block_size\n# When creating batches in practice we could need padding token and if pad left or right!\n# Also how to shuffle if we need to if data does not fit in memory: https://blog.janestreet.com/how-to-shuffle-a-big-dataset/\nx = train_data[:block_size]\ny = train_data[1:block_size + 1]\nfor t in range(block_size):\n    context = x[:t + 1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n\nwhen input is tensor([18]) the target: 47\nwhen input is tensor([18, 47]) the target: 56\nwhen input is tensor([18, 47, 56]) the target: 57\nwhen input is tensor([18, 47, 56, 57]) the target: 58\nwhen input is tensor([18, 47, 56, 57, 58]) the target: 1\nwhen input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n\n\n\ntorch.manual_seed(1337)\nbatch_size = 4  # how many independent sequences will we process in parallel?\nblock_size = 8  # what is the maximum context length for predictions?\n\n\ndef get_batch(split_kind: str) -&gt; tuple[Int64[Array, \"n_batches block_size\"], Int64[Array, \"n_batches block_size\"]]:\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split_kind == 'train' else val_data\n    # Random starting indices of blocks, notice that blocks can overlap\n    # To do something like this for real: \n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i + block_size] for i in ix])\n    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n    return x, y\n\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('----')\n\nfor b in range(batch_size):  # batch dimension\n    for t in range(block_size):  # time dimension\n        context = xb[b, :t + 1]\n        target = yb[b, t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n\ninputs:\ntorch.Size([4, 8])\ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\ntargets:\ntorch.Size([4, 8])\ntensor([[43, 58,  5, 57,  1, 46, 43, 39],\n        [53, 56,  1, 58, 46, 39, 58,  1],\n        [58,  1, 58, 46, 39, 58,  1, 46],\n        [17, 27, 10,  0, 21,  1, 54, 39]])\n----\nwhen input is [24] the target: 43\nwhen input is [24, 43] the target: 58\nwhen input is [24, 43, 58] the target: 5\nwhen input is [24, 43, 58, 5] the target: 57\nwhen input is [24, 43, 58, 5, 57] the target: 1\nwhen input is [24, 43, 58, 5, 57, 1] the target: 46\nwhen input is [24, 43, 58, 5, 57, 1, 46] the target: 43\nwhen input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\nwhen input is [44] the target: 53\nwhen input is [44, 53] the target: 56\nwhen input is [44, 53, 56] the target: 1\nwhen input is [44, 53, 56, 1] the target: 58\nwhen input is [44, 53, 56, 1, 58] the target: 46\nwhen input is [44, 53, 56, 1, 58, 46] the target: 39\nwhen input is [44, 53, 56, 1, 58, 46, 39] the target: 58\nwhen input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52] the target: 58\nwhen input is [52, 58] the target: 1\nwhen input is [52, 58, 1] the target: 58\nwhen input is [52, 58, 1, 58] the target: 46\nwhen input is [52, 58, 1, 58, 46] the target: 39\nwhen input is [52, 58, 1, 58, 46, 39] the target: 58\nwhen input is [52, 58, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\nwhen input is [25] the target: 17\nwhen input is [25, 17] the target: 27\nwhen input is [25, 17, 27] the target: 10\nwhen input is [25, 17, 27, 10] the target: 0\nwhen input is [25, 17, 27, 10, 0] the target: 21\nwhen input is [25, 17, 27, 10, 0, 21] the target: 1\nwhen input is [25, 17, 27, 10, 0, 21, 1] the target: 54\nwhen input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n\n\n\nprint(xb)  # our input to the transformer\n\ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\n\n\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\ntorch.manual_seed(1337)\n\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size: int):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx: Long[torch.Tensor, \"batch_dim context_dim\"],\n                targets: Optional[Long[torch.Tensor, \"batch_dim context_dim\"]] = None):\n\n        # idx and targets are both (B,T) tensor of integers\n        logits: Long[torch.Tensor, \"batch_dim context_dim latent_dim\"] = self.token_embedding_table(\n            idx)  # (B,T,C=vocab_size)\n\n        if targets is None:\n            return logits, None\n        else:\n            # Note that here strictly speaking this does not fix batch size explicitly to B\n            B, T, C = logits.shape  # (B,T,C=vocab_size)\n            # Just a hack to avoid transposing, cross_entropy expects B x C x T in batched mode\n            # This converts into non batched mode\n            logits: Long[torch.Tensor, \"batch_dim*context_dim latent_dim\"] = logits.view(B * T, C)\n            targets: Long[torch.Tensor, \"batch_dim*context_dim\"] = targets.view(B * T)\n            # https://agustinus.kristia.de/techblog/2016/12/21/forward-reverse-kl/\n            loss: Float[torch.Tensor, \"\"] = F.cross_entropy(logits, targets)\n            return logits, loss\n\n    def generate(self, idx: Long[torch.Tensor, \"batch_dim context_dim\"], max_new_tokens: int):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)\n            # focus only on the last time step\n            logits = logits[:, -1, :]  # becomes (B, vocab_size)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1)  # (B, vocab_size)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n            # What can go wrong here? and it is not handled at all\n        return idx\n\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\nprint(loss.shape)\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n\ntorch.Size([32, 65])\ntensor(4.8786, grad_fn=&lt;NllLossBackward0&gt;)\ntorch.Size([])\n\nSr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n\n\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n# weighted average of mean and \"variance\" of gradients +  a trick\n# Not feasible for deep learning, too costly to store harder to parallelize!\n# https://en.wikipedia.org/wiki/Limited-memory_BFGS approximates inverse of Hessian implicitly and even this is too costly\n# https://en.wikipedia.org/wiki/Wolfe_conditions\n# https://en.wikipedia.org/wiki/Backtracking_line_search more advanced step size\n\n\nbatch_size = 32\nn_steps = 1_000\nfor steps in range(n_steps):  # increase number of steps for good results...\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n\n3.7218432426452637\n\n\n\nprint(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n\n\nolylvLLko'TMyatyIoconxad.?-tNSqYPsx&bF.oiR;BD$dZBMZv'K f bRSmIKptRPly:AUC&$zLK,qUEy&Ay;ZxjKVhmrdagC-bTop-QJe.H?x\nJGF&pwst-P sti.hlEsu;w:w a BG:tLhMk,epdhlay'sVzLq--ERwXUzDnq-bn czXxxI&V&Pynnl,s,Ioto!uvixwC-IJXElrgm C-.bcoCPJ\nIMphsevhO AL!-K:AIkpre,\nrPHEJUzV;P?uN3b?ohoRiBUENoV3B&jumNL;Aik,\nxf -IEKROn JSyYWW?n 'ay;:weO'AqVzPyoiBL? seAX3Dot,iy.xyIcf r!!ul-Koi:x pZrAQly'v'a;vEzN\nBwowKo'MBqF$PPFb\nCjYX3beT,lZ qdda!wfgmJP\nDUfNXmnQU mvcv?nlnQF$JUAAywNocd  bGSPyAlprNeQnq-GRSVUP.Ja!IBoDqfI&xJM AXEHV&DKvRS"
  },
  {
    "objectID": "posts/24_05_2024_gpt2_from_scratch/gpt_dev.html#the-mathematical-trick-in-self-attention",
    "href": "posts/24_05_2024_gpt2_from_scratch/gpt_dev.html#the-mathematical-trick-in-self-attention",
    "title": "GPT from scratch with comments",
    "section": "",
    "text": "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\ntorch.manual_seed(42)\n# Causal attention does not take into account future information\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0, 10, (3, 2)).float()\nc = a @ b\nprint('a=')\nprint(a)\nprint('--')\nprint('b=')\nprint(b)\nprint('--')\nprint('c=')\nprint(c)\n\na=\ntensor([[1.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000],\n        [0.3333, 0.3333, 0.3333]])\n--\nb=\ntensor([[2., 7.],\n        [6., 4.],\n        [6., 5.]])\n--\nc=\ntensor([[2.0000, 7.0000],\n        [4.0000, 5.5000],\n        [4.6667, 5.3333]])\n\n\n\n# consider the following toy example:\n\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 2  # batch, time, channels\nx = torch.randn(B, T, C)\nx.shape\n\ntorch.Size([4, 8, 2])\n\n\n\n# We want $x[b,t] = mean_{i&lt;=t} x[b,i]$\nxbow = torch.zeros((B, T, C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b, :t + 1]  # (t,C)\n        xbow[b, t] = torch.mean(xprev, 0)\n\n\n# version 2: using matrix multiply for a weighted aggregation\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxbow2 = wei @ x  # (B, T, T) @ (B, T, C) ----&gt; (B, T, C)\ntorch.allclose(xbow, xbow2)\n\nFalse\n\n\n\n# version 3: use Softmax\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\n# This seems strange at first glance but exp(-inf) = 0 and it is well defined, otherwise training would get broken constantly\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n\nFalse\n\n\n\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 32  # batch, time, channels\nx = torch.randn(B, T, C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)  # (B, T, 16)\nq = query(x)  # (B, T, 16)\nwei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---&gt; (B, T, T)\n\ntril = torch.tril(torch.ones(T, T))\n# wei = torch.zeros((T,T))\n# Full matrix multiplication is faster, just block what we do not need\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n# out = wei @ x\n\nout.shape\n\ntorch.Size([4, 8, 16])\n\n\n\nwei[0]\n\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n       grad_fn=&lt;SelectBackward0&gt;)\n\n\nNotes: - Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights. - There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens. - Each example across batch dimension is of course processed completely independently and never “talk” to each other - In an “encoder” attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a “decoder” attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling. - “self-attention” just means that the keys and values are produced from the same source as queries. In “cross-attention”, the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module) - “Scaled” attention additionally divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below\n\n# head_size is the dimension of \"latent space\"\nk = torch.randn(B, T, head_size)\nq = torch.randn(B, T, head_size)\nwei = q @ k.transpose(-2, -1) * head_size ** -0.5\n\n\nk.var()\n\ntensor(1.0449)\n\n\n\nq.var()\n\ntensor(1.0700)\n\n\n\nwei.var()\n\ntensor(1.0918)\n\n\n\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)\n\ntensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n\n\n\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]) * 8, dim=-1)  # gets too peaky, converges to one-hot\n\ntensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n\n\n\nclass LayerNorm1d:  # (used to be BatchNorm1d)\n\n    def __init__(self, dim, eps=1e-5, momentum=0.1):\n        self.eps = eps\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n\n    def __call__(self, x):\n        # calculate the forward pass\n        xmean = x.mean(1, keepdim=True)  # batch mean\n        xvar = x.var(1, keepdim=True)  # batch variance\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n        self.out = self.gamma * xhat + self.beta\n        return self.out\n\n    def parameters(self):\n        return [self.gamma, self.beta]\n\n\ntorch.manual_seed(1337)\nmodule = LayerNorm1d(100)\n# batch_norm_1d = nn.BatchNorm1d(100)\n# x_normalized_torch = batch_norm_1d(x)\n\nx = torch.randn(32, 100)  # batch size 32 of 100-dimensional vectors\nx_normalized = module(x)\nx_normalized.shape\n\ntorch.Size([32, 100])\n\n\n\nx[:, 0].mean(), x[:, 0].std()  # mean,std of one feature across all batch inputs\n\n(tensor(0.1392), tensor(0.8899))\n\n\n\nx[0, :].mean(), x[0, :].std()  # mean,std of a single input from the batch, of its features\n\n(tensor(0.0409), tensor(1.0476))\n\n\n\n# French to English translation example:\n\n# &lt;--------- ENCODE ------------------&gt;&lt;--------------- DECODE -----------------&gt;\n# les réseaux de neurones sont géniaux! &lt;START&gt; neural networks are awesome!&lt;END&gt;\n\n\n\nYou may want to refer directly to the git repo instead though.\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 1024 # 16  # how many independent sequences will we process in parallel?\nblock_size = 128 # 32  # what is the maximum context length for predictions?\nmax_iters = 10000\neval_interval = 100\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 64\nn_head = 4\nn_layer = 4\ndropout = 0.0\n# ------------\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n# create a mapping from characters to integers\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9 * len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i + block_size] for i in ix])\n    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention\n        https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention\n    \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        # This limits us to the maximal context block_size\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)  # (B,T,C=n_embd) -&gt; (B,T,C=head_size)\n        q = self.query(x)  # (B,T,C=n_embd) -&gt; (B,T,C=head_size)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2, -1) * C ** -0.5  # (B, T, C=head_size) @ (B, C=head_size), T) -&gt; (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n        # The drop out is over full matrix, alternatively it would be better to just drop on mask, this is biased\n        # Also it seems maybe conceptually we should just do symmetric dropout\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x)  # (B,T,C=head_size)\n        # The matrix multiplication is batched and applied on last two dimensions!\n        out = wei @ v  # (B, T, T) @ (B, T, C=head_size) -&gt; (B, T, C=head_size)\n        return out\n\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        # Projection back to residual pathway, align the basis\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd), # Projection back to residual pathway\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd: int, n_head: int):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        # To guarantee that final concatenated embedding is of size n_embd \n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        # Skip connections added to improve the flow of gradient\n        # Need to project back to residual pathway in sa and ffwd to \"align bases\" \n        \n        # Modern way is do to layer norm before and not after, in original paper it was done after, we do before!\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\n\n# Not so super simple and not bigram model anymore\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token is mapped to latent space of size n_embd\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        # learnable position embedding for positions 0, ..., block_size - 1\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        # Modern way of handling this is:\n        # https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/\n        # https://afterhoursresearch.hashnode.dev/rope-rotary-positional-embedding\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx)  # (B,T,C=n_embd)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C=n_embd)\n        x = tok_emb + pos_emb  # (B,T,C=n_embd)\n        x = self.blocks(x)  # (B,T,C=n_embd)\n        # Modern way is do to layer norm before and not after, in original paper it was done after, we do before!\n        x = self.ln_f(x)  # (B,T,C=n_embd)\n        logits = self.lm_head(x)  # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B * T, C)\n            targets = targets.view(B * T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens, otherwise things will explode\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :]  # (B, T, C=vocab_size) becomes (B, C=vocab_size)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1)  # (B, C=vocab_size)\n            # sample from the multinomial distribution\n            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n        return idx\n\n\nmodel = BigramLanguageModel()\nm = model.to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in m.parameters()) / 1e6, 'M parameters')\n\n# create a PyTorch optimizer\n# https://www.ruder.io/optimizing-gradient-descent/#adam\n# https://iclr-blogposts.github.io/2023/blog/2023/adamw/\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n# Temperature motivation https://en.wikipedia.org/wiki/LogSumExp\n# What happens when you divide logits by posite Temp and then do softmax\n# Temp close to 0 is max in limit, one hot, Temp &gt;&gt; 1 in limit random\n\n0.215873 M parameters\nstep 0: train loss 4.3254, val loss 4.3198\nstep 100: train loss 2.5600, val loss 2.5620\nstep 200: train loss 2.4232, val loss 2.4276\nstep 300: train loss 2.2731, val loss 2.2882\nstep 400: train loss 2.1599, val loss 2.1881\nstep 500: train loss 2.0462, val loss 2.0977\nstep 600: train loss 1.9539, val loss 2.0254\nstep 700: train loss 1.8708, val loss 1.9669\nstep 800: train loss 1.8075, val loss 1.9265\nstep 900: train loss 1.7639, val loss 1.8996\nstep 1000: train loss 1.7138, val loss 1.8639\nstep 1100: train loss 1.6799, val loss 1.8366\nstep 1200: train loss 1.6495, val loss 1.8171\nstep 1300: train loss 1.6253, val loss 1.7901\nstep 1400: train loss 1.6056, val loss 1.7831\nstep 1500: train loss 1.5937, val loss 1.7649\nstep 1600: train loss 1.5675, val loss 1.7471\nstep 1700: train loss 1.5596, val loss 1.7430\nstep 1800: train loss 1.5432, val loss 1.7324\nstep 1900: train loss 1.5295, val loss 1.7198\nstep 2000: train loss 1.5209, val loss 1.7141\nstep 2100: train loss 1.5125, val loss 1.7100\nstep 2200: train loss 1.5057, val loss 1.7036\nstep 2300: train loss 1.4972, val loss 1.6989\nstep 2400: train loss 1.5017, val loss 1.7052\nstep 2500: train loss 1.4788, val loss 1.6915\nstep 2600: train loss 1.4783, val loss 1.6897\nstep 2700: train loss 1.4680, val loss 1.6796\nstep 2800: train loss 1.4690, val loss 1.6847\nstep 2900: train loss 1.4657, val loss 1.6848\nstep 3000: train loss 1.4583, val loss 1.6755\nstep 3100: train loss 1.4511, val loss 1.6711\nstep 3200: train loss 1.4423, val loss 1.6699\nstep 3300: train loss 1.4397, val loss 1.6653\nstep 3400: train loss 1.4379, val loss 1.6670\nstep 3500: train loss 1.4387, val loss 1.6658\nstep 3600: train loss 1.4324, val loss 1.6644\nstep 3700: train loss 1.4373, val loss 1.6753\nstep 3800: train loss 1.4368, val loss 1.6735\nstep 3900: train loss 1.4280, val loss 1.6606\nstep 4000: train loss 1.4182, val loss 1.6546\nstep 4100: train loss 1.4152, val loss 1.6576\nstep 4200: train loss 1.4103, val loss 1.6493\nstep 4300: train loss 1.4107, val loss 1.6541\nstep 4400: train loss 1.4070, val loss 1.6480\nstep 4500: train loss 1.4079, val loss 1.6546\nstep 4600: train loss 1.4007, val loss 1.6493\nstep 4700: train loss 1.4011, val loss 1.6473\nstep 4800: train loss 1.3991, val loss 1.6462\nstep 4900: train loss 1.3944, val loss 1.6454\nstep 5000: train loss 1.3910, val loss 1.6423\nstep 5100: train loss 1.3937, val loss 1.6460\nstep 5200: train loss 1.3922, val loss 1.6466\nstep 5300: train loss 1.3891, val loss 1.6476\nstep 5400: train loss 1.3853, val loss 1.6426\nstep 5500: train loss 1.3806, val loss 1.6390\nstep 5600: train loss 1.3917, val loss 1.6519\nstep 5700: train loss 1.3813, val loss 1.6436\nstep 5800: train loss 1.3840, val loss 1.6437\nstep 5900: train loss 1.3801, val loss 1.6430\nstep 6000: train loss 1.3750, val loss 1.6401\nstep 6100: train loss 1.3709, val loss 1.6355\nstep 6200: train loss 1.3721, val loss 1.6365\nstep 6300: train loss 1.3685, val loss 1.6341\nstep 6400: train loss 1.3709, val loss 1.6338\nstep 6500: train loss 1.3636, val loss 1.6296\nstep 6600: train loss 1.3659, val loss 1.6315\nstep 6700: train loss 1.3622, val loss 1.6283\nstep 6800: train loss 1.3635, val loss 1.6318\nstep 6900: train loss 1.3647, val loss 1.6374\nstep 7000: train loss 1.3653, val loss 1.6358\nstep 7100: train loss 1.3614, val loss 1.6366\nstep 7200: train loss 1.3581, val loss 1.6324\nstep 7300: train loss 1.3528, val loss 1.6317\nstep 7400: train loss 1.3544, val loss 1.6292\nstep 7500: train loss 1.3523, val loss 1.6288\nstep 7600: train loss 1.3500, val loss 1.6294\nstep 7700: train loss 1.3504, val loss 1.6278\nstep 7800: train loss 1.3503, val loss 1.6281\nstep 7900: train loss 1.3431, val loss 1.6207\nstep 8000: train loss 1.3466, val loss 1.6252\nstep 8100: train loss 1.3486, val loss 1.6304\nstep 8200: train loss 1.3413, val loss 1.6257\nstep 8300: train loss 1.3434, val loss 1.6233\nstep 8400: train loss 1.3417, val loss 1.6285\nstep 8500: train loss 1.3397, val loss 1.6190\nstep 8600: train loss 1.3404, val loss 1.6283\nstep 8700: train loss 1.3382, val loss 1.6265\nstep 8800: train loss 1.3364, val loss 1.6270\nstep 8900: train loss 1.3345, val loss 1.6193\nstep 9000: train loss 1.3342, val loss 1.6248\nstep 9100: train loss 1.3363, val loss 1.6275\nstep 9200: train loss 1.3332, val loss 1.6228\nstep 9300: train loss 1.3345, val loss 1.6238\nstep 9400: train loss 1.3334, val loss 1.6260\nstep 9500: train loss 1.3359, val loss 1.6290\nstep 9600: train loss 1.3313, val loss 1.6245\nstep 9700: train loss 1.3268, val loss 1.6246\nstep 9800: train loss 1.3321, val loss 1.6256\nstep 9900: train loss 1.3272, val loss 1.6196\nstep 9999: train loss 1.3266, val loss 1.6243\n\n\nYORK:\nI beggan in This name King Peter, and envy\ngrades the change art that us he had beck:\nAs was, away, my facts arrizons heavens,\nHow is he cover heldly, and if the neck. Take my lieve to the\nWill may in wabgunel lind tear.\n\nAUFIDIUS:\nWhat! O have plaw you:'ld me not\nTo tell I will the shamed again Willohed their king.\n\nGRUMIO:\nEven his no the comes for under danger,\nWhich arms you must with all on,\nTo fight? grant nor furscience you acharding again,\nFor hither to mark no Rutland, and for his needy out\nIs yourselves gatesman; a they arms? what the men.\nWas mayst thou sworn to the dead of cour speaks\nFor mighty my castle lack and sorted and hellow.\nLading to Leond more over Heifts, upon thou fest.\nAs not controves, she shall look to entrough.\n\nKING HENRY VI:\nHeaving with kill it bump whose told lady.\n\nProvost:\nHe had abhold. I know the no none Captuluse: I do it is\nouthfunance eyes and approach'd when no mother.\n\nGLOUCESTER:\nHoour lord, why, because I, thus wounder for Gloucester.\nYet I am a runse. This I husband; go whil though,\nLook not take all the went should men,\nTrue thinks and endly in it infected greates,\nBut now, call King Henry's fatnerment their\nOf prayer be any inkering lament tell thee expection not myself.\n\nBUCKINGHAM:\nThe time for Hereford!\nThen lands! then often foolish'd her wantsrified?\n\nSecond Kench English clock?\n\nKING EDWARD IV:\nMy love the begun,\nGood queen the to the lave's else of my lorious numblemen'd,\nThe people, age finger I attend are holess!\n\nROMEO:\nIf they shut be the shall, as I tho\nmatter virtuce!\nCountry thee till thee world, she worse, who drink?\n\nMARCIUS:\nLet fortune.\n\nBENVOLIO:\nI am wound are to-marriage,\nBeath, could hither of some to, my hearth; can\nhard I'ld not father.\n\nLUCIO:\nI have power, Hastings Lanca.\n\nPRINCE:\nOur whiptee, before a gentle!\n\nCLARENCE:\n'Fitter like he punish'd, go to the autch'd:\nAnd fancy you, yet it an any any sent,\nBut battle comfort, Juliet pardon.\n\nISABELLA:\nO'I fight he to the good the world and u\n\n\n\n# https://github.com/szagoruyko/pytorchviz\n# https://github.com/mert-kurttutan/torchview nicer graph\n# https://github.com/mert-kurttutan/torchview\nfrom torchview import draw_graph\nimport torchlens as tl\n\n# device='meta' -&gt; no memory is consumed for visualization\nprint(xb.shape)\nprint(yb.shape)\nmodel_graph = draw_graph(model, input_data=(xb, yb), device='cuda')\nmodel_graph.visual_graph\n\n# model_history = tl.log_forward_pass(model, (xb, yb)\n#                                      , layers_to_save='all', vis_opt='unrolled')\n# print(model_history)\n\nNameError: name 'model_history' is not defined\n\n\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(model.generate(context, max_new_tokens=2000)[0].tolist()))\n\n\nCity and by his warms, went's scomptude, that\ntill ragge untong-make's.' But, what a determy, thou\nfaintors, you attend underpt my mother.\n\nSecond Men, an well.\nCome I mean as thing once with I recond this;\nAnd I may of looks to a plenity,\nAnd fears 'free and joy myself,\nWhen gazes liege titles wise and my restraint.\n\nKING EDWARD IV:\nWhoel, she was this head.\n\nDUKE OF YORK:\nThen frame thyself? for he forth and in this.\nWere commend in the fiends, o' the voice\nMy mother counsished, to two Ely; the counter\nThat I do. So an ever maid in beast plots.\n\nDUKE O FRIZWIIC John:\nUpon thine is kind; too, Tower; thou dust baldgiant\nAnd dost happille to the poore's doin work her,\nMarriabe.\n\nBENVOLIO:\nIf ends ye.\n\nSICINIUS:\nGood love I had unto conveys,\nTo my sovereign i' the dove me.\n\nHENRY BOLINGHAM:\nWere glast you shall be as the cannot both:\nThou art me in my consent until thee,\nOr thee to direct against partiest;\nBut! seeing she parks of cross on me age good\nSo queen will answering doth chase, letter.\nSo, that you on my lady.\n\nDUKE OF AUMERLE:\nNo fellow and Romeo Edward, willingly of God,\nAnd Bonding when the sistern he widow;\nFor hung else hour tope head up this converses!\nAway she, who is, I'll same of brought,\nArt should, that now their father?\n\nPOMPEY:\nSicile you will: have you were not speak to the blot,\nDisself that winder my lady me.\nand loya 'twas your field this fathes,\nI shall my fortune joys and I cit thou are;\nThen wounds speak them not speak.\nIf God, my lord, as it is this many coundation,\nFor and crowned the acce.\n\nMENENIUS:\nWay, pardon, his patience;\nWill give and we may bring you disdain's basters.\nYou threw attailymy. Go traitor were.\n\nSARMIANA:\nNot before; mannish wife?\n\nCOMINIUS:\nLift--\n\nGLOUCESTER:\nWhy villain again, I'll make you misber in tent\nRedly, listeen heath will his us were of senate\nIs no amient look'd yet a little.\n\nMENENIUS:\nI did his body throwed of this?\n\nPage:\nMy womb, Edward house will and deserve it.\n\nLEONTES:\nIt you it do.\n\nDUKE VINCENTI"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code, just playing around."
  }
]